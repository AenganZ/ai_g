# pseudonymization/core.py
"""
ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ í•µì‹¬ ê°€ëª…í™” í†µí•© ëª¨ë“ˆ
í† í° ê¸°ë°˜ ê°€ëª…í™” â†’ AI ì²˜ë¦¬ â†’ í† í° ë³µì›
"""

import time
from typing import Dict, Any, List
from .pools import initialize_pools, get_pools
from .detection import detect_pii_enhanced
from .replacement import get_workflow_manager, apply_tokenization, restore_from_tokens, create_detailed_mapping_report

# ì›Œí¬í”Œë¡œìš° í•µì‹¬ í•¨ìˆ˜ export
__all__ = [
    'pseudonymize_text',
    'restore_original', 
    'workflow_process_ai_response',
    'load_data_pools',
    'get_data_pool_stats',
    'assign_realistic_values',
    'create_masked_text'
]

def pseudonymize_text(text: str, detailed_report: bool = True) -> Dict[str, Any]:
    """
    ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ í†µí•© ê°€ëª…í™” í•¨ìˆ˜
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        detailed_report: ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± ì—¬ë¶€
        
    Returns:
        dict: ê°€ëª…í™” ê²°ê³¼ (í† í°í™”ëœ í…ìŠ¤íŠ¸ í¬í•¨)
    """
    start_time = time.time()
    print(f"ğŸš€ ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ê°€ëª…í™” ì‹œì‘: {text[:50]}...")
    
    # ë°ì´í„°í’€ í™•ì¸ ë° ì´ˆê¸°í™”
    pools = get_pools()
    if not pools._initialized:
        print("ğŸ“¦ ë°ì´í„°í’€ ì´ˆê¸°í™” ì¤‘...")
        initialize_pools()
    
    # PII íƒì§€ (ì›Œí¬í”Œë¡œìš° ê¸°ë°˜)
    print("ğŸ” PII íƒì§€ (ì›Œí¬í”Œë¡œìš° ê¸°ë°˜)")
    detection = detect_pii_enhanced(text)
    
    if not detection['items']:
        processing_time = time.time() - start_time
        print(f"âŒ PII íƒì§€ë˜ì§€ ì•ŠìŒ. ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
        return {
            "original": text,
            "pseudonymized": text,
            "pseudonymized_text": text,
            "tokenized_text": text,  # ì›Œí¬í”Œë¡œìš°ìš©
            "masked_prompt": text,
            "detection": detection,
            "substitution_map": {},
            "reverse_map": {},
            "token_map": {},  # ì›Œí¬í”Œë¡œìš°ìš©
            "mapping_report": "PIIê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "processing_time": processing_time,
            "stats": {
                "detected_items": 0,
                "replaced_items": 0,
                "detection_time": processing_time,
                "replacement_time": 0,
                "total_time": processing_time,
                "items_by_type": {},
                "detection_stats": detection['stats']
            }
        }
    
    # í† í° ê¸°ë°˜ ì¹˜í™˜ ì²˜ë¦¬
    detection_time = time.time() - start_time
    replacement_start = time.time()
    
    manager = get_workflow_manager()
    token_map = detection['stats']['token_map']
    substitution_map, reverse_map = manager.create_substitution_map(detection['items'], token_map)
    
    # í…ìŠ¤íŠ¸ í† í°í™”
    tokenized_text = apply_tokenization(text, substitution_map)
    
    replacement_time = time.time() - replacement_start
    total_time = time.time() - start_time
    
    # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    mapping_report = ""
    if detailed_report:
        mapping_report = create_detailed_mapping_report(substitution_map, reverse_map)
    
    print(f"ğŸ“ ì´ì „: {text}")
    print(f"ğŸ·ï¸ í† í°í™”: {tokenized_text}")
    print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: íƒì§€ {detection_time:.3f}ì´ˆ, í† í°í™” {replacement_time:.3f}ì´ˆ, ì „ì²´ {total_time:.3f}ì´ˆ")
    
    # ê²°ê³¼ ë°˜í™˜
    result = {
        "original": text,
        "pseudonymized": tokenized_text,  # í† í°í™”ëœ í…ìŠ¤íŠ¸
        "pseudonymized_text": tokenized_text,  # í˜¸í™˜ì„±
        "tokenized_text": tokenized_text,  # ì›Œí¬í”Œë¡œìš°ìš© (AIë¡œ ì „ì†¡í•  í…ìŠ¤íŠ¸)
        "masked_prompt": tokenized_text,  # í˜¸í™˜ì„±
        "detection": detection,
        "substitution_map": substitution_map,  # ì›ë³¸ â†’ í† í°
        "reverse_map": reverse_map,  # í† í° â†’ ì›ë³¸
        "token_map": token_map,  # ì›Œí¬í”Œë¡œìš°ìš©
        "mapping_report": mapping_report,
        "processing_time": total_time,
        "stats": {
            "detected_items": len(detection['items']),
            "replaced_items": len(substitution_map),
            "detection_time": detection_time,
            "replacement_time": replacement_time,
            "total_time": total_time,
            "items_by_type": detection['stats']['items_by_type'],
            "detection_stats": detection['stats']['detection_stats']
        }
    }
    
    return result

def restore_original(tokenized_text: str, reverse_map: Dict[str, str]) -> str:
    """í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ìœ¼ë¡œ ë³µì› (ì›Œí¬í”Œë¡œìš° 4ë‹¨ê³„)"""
    print("ğŸ”„ ì›Œí¬í”Œë¡œìš° 4ë‹¨ê³„: AI ì‘ë‹µ ë³µì›")
    return restore_from_tokens(tokenized_text, reverse_map)

def workflow_process_ai_response(ai_response: str, reverse_map: Dict[str, str]) -> str:
    """ì›Œí¬í”Œë¡œìš° 4ë‹¨ê³„: AI ì‘ë‹µì„ ë³µì›í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
    
    print("ğŸ¤– AI ì‘ë‹µ ìˆ˜ì‹  ë° ë³µì› ì‹œì‘")
    print(f"ğŸ¤– AI ì‘ë‹µ (í† í°í™”ë¨): {ai_response[:100]}...")
    
    # AI ì‘ë‹µì—ì„œ í† í°ì„ ì›ë³¸ìœ¼ë¡œ ë³µì›
    restored_response = restore_from_tokens(ai_response, reverse_map)
    
    print(f"âœ… ë³µì›ëœ ìµœì¢… ë‹µë³€: {restored_response[:100]}...")
    
    return restored_response

def load_data_pools(custom_data: Dict = None):
    """ë°ì´í„°í’€ ë¡œë“œ"""
    print("ğŸ“¦ ë°ì´í„°í’€ ë¡œë”© ì¤‘...")
    initialize_pools(custom_data)
    print("ğŸ“¦ ë°ì´í„°í’€ ë¡œë”© ì™„ë£Œ")

def get_data_pool_stats() -> Dict[str, int]:
    """ë°ì´í„°í’€ í†µê³„ ì •ë³´"""
    pools = get_pools()
    return {
        "íƒì§€_ì´ë¦„ìˆ˜": len(pools.real_names),
        "íƒì§€_ì£¼ì†Œìˆ˜": len(pools.real_addresses),
        "íƒì§€_ë„ë¡œìˆ˜": len(pools.road_names),
        "íƒì§€_ì‹œêµ°êµ¬ìˆ˜": len(pools.districts),
        "íƒì§€_ì‹œë„ìˆ˜": len(pools.provinces),
        "íšŒì‚¬ìˆ˜": len(pools.companies)
    }

# í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ë“¤
def assign_realistic_values(items: List[Dict[str, Any]]) -> Dict[str, str]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    manager = get_workflow_manager()
    
    # ê°„ë‹¨í•œ í† í° ë§µ ìƒì„±
    token_map = {}
    for i, item in enumerate(items):
        token_map[item['value']] = f"[ITEM_{i}]"
    
    substitution_map, _ = manager.create_substitution_map(items, token_map)
    return substitution_map

def create_masked_text(text: str, substitution_map: Dict[str, str]) -> str:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return apply_tokenization(text, substitution_map)