# pseudonymization/detection.py
"""
ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ PII íƒì§€ ëª¨ë“ˆ
1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤
2ì°¨: NER ë³´ê°• (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ, ë†’ì€ ì„ê³„ì¹˜)
ì¹˜í™˜ í† í°: [PER_0], [ORG_0], [LOC_0] ë“±
"""

import re
import asyncio
import time
from typing import List, Dict, Any, Set

# ì •ê·œì‹ íŒ¨í„´ (1ì°¨ ê³ ì† íŒ¨ìŠ¤)
EMAIL_PATTERN = re.compile(r'\S+@\S+\.\S+')
PHONE_PATTERN = re.compile(
    r'(?:010|011|016|017|018|019|02|031|032|033|041|042|043|044|051|052|053|054|055|061|062|063|064)'
    r'[-.\s]?\d{3,4}[-.\s]?\d{4}'
)
AGE_PATTERN = re.compile(r'(\d{1,3})\s*(?:ì„¸|ì‚´)')
RRN_PATTERN = re.compile(r'\d{6}[-\s]?\d{7}')
CARD_PATTERN = re.compile(r'\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}')
ACCOUNT_PATTERN = re.compile(r'\d{10,16}')  # ê³„ì¢Œë²ˆí˜¸

# íŠ¹ë³„ ì²˜ë¦¬ê°€ í•„ìš”í•œ ì§€ì—­
SPECIAL_CITIES = {
    'ëŒ€êµ¬': 'city',  # ëŒ€êµ¬ëŠ” ì‹œì´ì§€ë§Œ "êµ¬"ë¡œ ëë‚¨
    'ëŒ€ì „': 'city',
    'ë¶€ì‚°': 'city',
    'ì„œìš¸': 'city',
    'ì¸ì²œ': 'city',
    'ê´‘ì£¼': 'city',
    'ìš¸ì‚°': 'city'
}

def detect_with_regex_fast(text: str) -> List[Dict[str, Any]]:
    """1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤"""
    items = []
    
    print("ğŸš€ 1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤")
    
    # ì´ë©”ì¼ íƒì§€
    for match in EMAIL_PATTERN.finditer(text):
        items.append({
            "type": "ì´ë©”ì¼",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "ê·œì¹™-ì´ë©”ì¼"
        })
        print(f"ğŸ“§ ì´ë©”ì¼ íƒì§€: '{match.group()}'")
    
    # ì „í™”ë²ˆí˜¸ íƒì§€
    for match in PHONE_PATTERN.finditer(text):
        items.append({
            "type": "ì „í™”ë²ˆí˜¸",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "ê·œì¹™-ì „í™”ë²ˆí˜¸"
        })
        print(f"ğŸ“ ì „í™”ë²ˆí˜¸ íƒì§€: '{match.group()}'")
    
    # ë‚˜ì´ íƒì§€
    for match in AGE_PATTERN.finditer(text):
        age_value = match.group(1)
        age_num = int(age_value)
        if 1 <= age_num <= 120:
            items.append({
                "type": "ë‚˜ì´",
                "value": age_value,
                "start": match.start(),
                "end": match.start() + len(age_value),
                "confidence": 1.0,
                "source": "ê·œì¹™-ë‚˜ì´"
            })
            print(f"ğŸ‚ ë‚˜ì´ íƒì§€: '{age_value}'")
    
    # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ íƒì§€
    for match in RRN_PATTERN.finditer(text):
        items.append({
            "type": "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "ê·œì¹™-ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸"
        })
        print(f"ğŸ†” ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ íƒì§€: '{match.group()}'")
    
    # ì‹ ìš©ì¹´ë“œ íƒì§€
    for match in CARD_PATTERN.finditer(text):
        items.append({
            "type": "ì‹ ìš©ì¹´ë“œ",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "ê·œì¹™-ì‹ ìš©ì¹´ë“œ"
        })
        print(f"ğŸ’³ ì‹ ìš©ì¹´ë“œ íƒì§€: '{match.group()}'")
    
    print(f"ğŸš€ 1ì°¨ ê³ ì† íŒ¨ìŠ¤ ì™„ë£Œ: {len(items)}ê°œ íƒì§€")
    return items

def detect_names_with_realname_list(text: str) -> List[Dict[str, Any]]:
    """ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ ì´ë¦„ íƒì§€"""
    items = []
    
    print("ğŸ‘¤ ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ ì´ë¦„ íƒì§€")
    
    # ì¼ë°˜ì ì¸ í•œêµ­ì–´ ì´ë¦„ ëª©ë¡ (í™•ì¥)
    common_korean_names = [
        'ê¹€ì² ìˆ˜', 'ì´ì˜í¬', 'ë°•ë¯¼ìˆ˜', 'ìµœì˜ìˆ˜', 'ì •ë¯¼ì¤€', 'ê°•ì„œìœ¤', 'ì¡°ì§€ìš°', 'ìœ¤ì„œí˜„',
        'ì¥í•˜ì€', 'ì„ì˜ˆì€', 'í•œì§€ë¯¼', 'ì˜¤ìœ¤ì„œ', 'ì„œí•˜ìœ¤', 'ì‹ ì±„ì›', 'ê¶Œì§€ì›', 'í™©ìˆ˜ë¹ˆ',
        'ì•ˆë‹¤ì€', 'ì†¡ì˜ˆë¦°', 'ë¥˜ì‹œì€', 'ì „ì†Œì€', 'í™ê¸¸ë™', 'ê¹€ì˜í¬', 'ì´ì² ìˆ˜', 'ë°•ì˜ìˆ˜',
        'ìµœë¯¼ìˆ˜', 'ì •ì˜í¬', 'ê°•ì² ìˆ˜', 'ì¡°ì˜ìˆ˜', 'ìœ¤ë¯¼ìˆ˜', 'ì¥ì˜í¬', 'ì„ì² ìˆ˜', 'í•œì˜ìˆ˜',
        'ê¹€ë¯¼ì¤€', 'ì´ì„œì¤€', 'ë°•ë„ìœ¤', 'ìµœì˜ˆì¤€', 'ì •ì‹œìš°', 'ê°•ì£¼ì›', 'ì¡°í•˜ì¤€', 'ìœ¤ì§€í˜¸',
        'ì¥ì¤€ì„œ', 'ì„ê±´ìš°', 'í•œí˜„ìš°', 'ì˜¤ìš°ì§„', 'ì„œì„ ìš°', 'ì‹ ì—°ìš°', 'ê¶Œì •ìš°', 'í™©ì„±ë¯¼',
        'ê¹€ê°€ì˜', 'ì´ë‚˜ì˜', 'ë°•ìˆ˜ì˜', 'ìµœì§€ì˜', 'ì •ë¯¼ì˜', 'ê°•ìœ ì˜', 'ì¡°ì†Œì˜', 'ìœ¤ì€ì˜'
    ]
    
    # ì‹¤ëª… ëª©ë¡ì—ì„œ ì§ì ‘ ì°¾ê¸°
    for name in common_korean_names:
        if name in text:
            start_pos = text.find(name)
            items.append({
                "type": "ì´ë¦„",
                "value": name,
                "start": start_pos,
                "end": start_pos + len(name),
                "confidence": 0.95,
                "source": "ì‹¤ëª…ëª©ë¡"
            })
            print(f"ğŸ‘¤ ì‹¤ëª… íƒì§€: '{name}'")
    
    return items

def detect_names_with_patterns(text: str, exclude_detected: Set[str]) -> List[Dict[str, Any]]:
    """íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€"""
    items = []
    
    print("ğŸ” íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€")
    
    # ì´ë¦„ íŒ¨í„´ë“¤ (ë†’ì€ ì‹ ë¢°ë„)
    name_patterns = [
        r'([ê°€-í£]{2,4})ë‹˜(?!\w)',          # ì´ì˜í¬ë‹˜
        r'([ê°€-í£]{2,4})ì”¨(?!\w)',          # í™ê¸¸ë™ì”¨  
        r'ì´ë¦„ì€\s*([ê°€-í£]{2,4})(?!\w)',   # ì´ë¦„ì€ í™ê¸¸ë™
        r'ì €ëŠ”\s*([ê°€-í£]{2,4})(?!\w)',     # ì €ëŠ” í™ê¸¸ë™
        r'([ê°€-í£]{2,4})ì´ê³ (?!\w)',        # í™ê¸¸ë™ì´ê³ 
    ]
    
    # ì œì™¸í•  ë‹¨ì–´ (í™•ì¥)
    exclude_words = {
        'ê³ ê°', 'íšŒì›', 'ì‚¬ìš©ì', 'ê´€ë¦¬ì', 'ì§ì›', 'ë‹´ë‹¹ì', 'ì„ ìƒ', 'êµìˆ˜',
        'ë¶€ì¥', 'ê³¼ì¥', 'ëŒ€ë¦¬', 'íŒ€ì¥', 'ì‚¬ì¥', 'ëŒ€í‘œ', 'ì˜ˆì•½', 'í™•ì¸', 'ë¬¸ì˜',
        'ì—°ë½', 'ì£¼ì„¸', 'ìˆìŠµë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤'
    }
    
    for pattern in name_patterns:
        for match in re.finditer(pattern, text):
            name = match.group(1)
            
            # ì´ë¯¸ íƒì§€ëœ ì´ë¦„ì´ê±°ë‚˜ ì œì™¸ ë‹¨ì–´ë©´ ìŠ¤í‚µ
            if name in exclude_detected or name in exclude_words:
                continue
                
            # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
            if (len(name) >= 2 and 
                all(ord('ê°€') <= ord(char) <= ord('í£') for char in name)):
                
                items.append({
                    "type": "ì´ë¦„",
                    "value": name,
                    "start": match.start(1),
                    "end": match.end(1),
                    "confidence": 0.85,
                    "source": "íŒ¨í„´-ì´ë¦„"
                })
                print(f"ğŸ‘¤ íŒ¨í„´ ì´ë¦„ íƒì§€: '{name}'")
                exclude_detected.add(name)
    
    return items

def detect_addresses_smart(text: str) -> List[Dict[str, Any]]:
    """ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€ (ì¤‘ë³µ ë°©ì§€)"""
    items = []
    
    print("ğŸ  ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€")
    
    # ì£¼ìš” ë„ì‹œ (íŠ¹ë³„ ì²˜ë¦¬ í¬í•¨)
    cities = list(SPECIAL_CITIES.keys()) + ['ì„¸ì¢…', 'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼']
    
    # ì£¼ìš” êµ¬
    districts = [
        'ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì†¡íŒŒêµ¬', 'ê°•ë™êµ¬', 'ë§ˆí¬êµ¬', 'ìš©ì‚°êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬',
        'ê°•ì„œêµ¬', 'ì–‘ì²œêµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ë™ì‘êµ¬', 'ê´€ì•…êµ¬',
        'í•´ìš´ëŒ€êµ¬', 'ë¶€ì‚°ì§„êµ¬', 'ë™ë˜êµ¬', 'ìˆ˜ì˜êµ¬', 'ë‚¨êµ¬', 'ë¶êµ¬', 'ìˆ˜ì›ì‹œ', 'ì„±ë‚¨ì‹œ'
    ]
    
    detected_locations = []
    
    # ì‹œ/ë„ íƒì§€
    for city in cities:
        if city in text:
            for match in re.finditer(re.escape(city), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ",
                    "value": city,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.9,
                    "source": "íŒ¨í„´-ì£¼ì†Œ",
                    "location_type": "city"
                })
                print(f"ğŸ™ï¸ ë„ì‹œ íƒì§€: '{city}'")
    
    # êµ¬ íƒì§€ (ëŒ€êµ¬ ë“± íŠ¹ë³„ ì²˜ë¦¬)
    for district in districts:
        if district in text:
            # "ëŒ€êµ¬"ê°€ í¬í•¨ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if "ëŒ€êµ¬" in district and district != "ëŒ€êµ¬":
                continue  # "ëŒ€êµ¬"ëŠ” ì‹œì´ë¯€ë¡œ êµ¬ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                
            for match in re.finditer(re.escape(district), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ", 
                    "value": district,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.8,
                    "source": "íŒ¨í„´-ì£¼ì†Œ",
                    "location_type": "district"
                })
                print(f"ğŸ˜ï¸ êµ¬ íƒì§€: '{district}'")
    
    # ì¤‘ë³µ ì œê±°: ê²¹ì¹˜ëŠ” ìœ„ì¹˜ì˜ ì£¼ì†Œë“¤ ì¤‘ ì²« ë²ˆì§¸ë§Œ ì„ íƒ
    if detected_locations:
        # ì‹œì‘ ìœ„ì¹˜ë¡œ ì •ë ¬
        detected_locations.sort(key=lambda x: x['start'])
        
        # ì²« ë²ˆì§¸ ì£¼ì†Œë§Œ ì„ íƒ
        first_location = detected_locations[0]
        items.append(first_location)
        
        print(f"ğŸ  ì£¼ì†Œ ì¤‘ë³µ ì œê±°: {len(detected_locations)}ê°œ â†’ 1ê°œ")
        print(f"ğŸ  ì„ íƒëœ ì£¼ì†Œ: '{first_location['value']}'")
    
    return items

async def detect_with_ner_async(text: str, timeout: float = 0.1) -> List[Dict[str, Any]]:
    """2ì°¨: NER ë³´ê°• (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ, ë†’ì€ ì„ê³„ì¹˜)"""
    items = []
    
    try:
        print(f"ğŸ¤– 2ì°¨: NER ë³´ê°• (íƒ€ì„ì•„ì›ƒ: {timeout*1000:.0f}ms)")
        
        # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ NER ì‹¤í–‰
        async def run_ner():
            try:
                from .model import extract_entities_with_ner, is_ner_loaded
                
                if is_ner_loaded():
                    ner_items = extract_entities_with_ner(text)
                    
                    # ë†’ì€ ì„ê³„ì¹˜ ì ìš© (0.9 ì´ìƒ)
                    high_confidence_items = []
                    for item in ner_items:
                        if item['confidence'] > 0.9:
                            item['source'] = 'NER-ê³ ì‹ ë¢°ë„'
                            high_confidence_items.append(item)
                            print(f"ğŸ¤– NER ê³ ì‹ ë¢°ë„ íƒì§€: {item['type']} = '{item['value']}' (ì‹ ë¢°ë„: {item['confidence']:.3f})")
                    
                    return high_confidence_items
                else:
                    print("ğŸ¤– NER ëª¨ë¸ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                    return []
                    
            except Exception as e:
                print(f"ğŸ¤– NER ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return []
        
        # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì‹¤í–‰
        items = await asyncio.wait_for(run_ner(), timeout=timeout)
        print(f"ğŸ¤– 2ì°¨ NER ë³´ê°• ì™„ë£Œ: {len(items)}ê°œ íƒì§€")
        
    except asyncio.TimeoutError:
        print(f"ğŸ¤– NER íƒ€ì„ì•„ì›ƒ ({timeout*1000:.0f}ms) - ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©")
    except Exception as e:
        print(f"ğŸ¤– NER ë³´ê°• ì‹¤íŒ¨: {e}")
    
    return items

def merge_detections_with_priority(regex_items: List[Dict[str, Any]], 
                                  ner_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ , ìŠ¤íŒ¬ ì¶©ëŒ í•´ê²°)"""
    
    print("ğŸ”„ íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ )")
    
    # ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ê°€ ìš°ì„ 
    merged_items = regex_items.copy()
    
    # NER ê²°ê³¼ ì¶”ê°€ (ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
    for ner_item in ner_items:
        overlapped = False
        
        for regex_item in regex_items:
            # ìŠ¤íŒ¬ ì¶©ëŒ í™•ì¸
            if (ner_item['start'] < regex_item['end'] and 
                ner_item['end'] > regex_item['start']):
                overlapped = True
                print(f"ğŸ”„ ìŠ¤íŒ¬ ì¶©ëŒ ë¬´ì‹œ: NER '{ner_item['value']}' vs ê·œì¹™ '{regex_item['value']}'")
                break
        
        if not overlapped:
            merged_items.append(ner_item)
            print(f"ğŸ”„ NER ê²°ê³¼ ì¶”ê°€: '{ner_item['value']}'")
    
    # ì‹œì‘ ìœ„ì¹˜ë¡œ ì •ë ¬
    merged_items.sort(key=lambda x: x['start'])
    
    print(f"ğŸ”„ ë³‘í•© ì™„ë£Œ: ê·œì¹™ {len(regex_items)}ê°œ + NER {len(ner_items)}ê°œ â†’ {len(merged_items)}ê°œ")
    
    return merged_items

def assign_tokens(items: List[Dict[str, Any]]) -> Dict[str, str]:
    """ì¹˜í™˜ í† í° í• ë‹¹ ([PER_0], [ORG_0], [LOC_0] ë“±)"""
    
    print("ğŸ·ï¸ ì¹˜í™˜ í† í° í• ë‹¹")
    
    # íƒ€ì…ë³„ ì¹´ìš´í„°
    type_counters = {}
    token_map = {}
    
    # íƒ€ì…ë³„ í† í° ì ‘ë‘ì‚¬
    type_prefixes = {
        'ì´ë¦„': 'PER',
        'íšŒì‚¬': 'ORG', 
        'ì£¼ì†Œ': 'LOC',
        'ì´ë©”ì¼': 'EMAIL',
        'ì „í™”ë²ˆí˜¸': 'PHONE',
        'ë‚˜ì´': 'AGE',
        'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸': 'RRN',
        'ì‹ ìš©ì¹´ë“œ': 'CARD',
        'ê³„ì¢Œë²ˆí˜¸': 'ACCT'
    }
    
    for item in items:
        pii_type = item['type']
        pii_value = item['value']
        
        # íƒ€ì…ë³„ ì¹´ìš´í„° ì¦ê°€
        if pii_type not in type_counters:
            type_counters[pii_type] = 0
        
        # í† í° ìƒì„±
        prefix = type_prefixes.get(pii_type, 'MISC')
        token = f"[{prefix}_{type_counters[pii_type]}]"
        
        token_map[pii_value] = token
        type_counters[pii_type] += 1
        
        print(f"ğŸ·ï¸ {pii_value} â†’ {token}")
    
    print(f"ğŸ·ï¸ í† í° í• ë‹¹ ì™„ë£Œ: {len(token_map)}ê°œ")
    
    return token_map

def detect_pii_enhanced(text: str) -> Dict[str, Any]:
    """ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ê°•í™”ëœ PII íƒì§€"""
    
    print("=" * 60)
    print("ğŸ” ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ PII íƒì§€ ì‹œì‘")
    print("=" * 60)
    
    # 1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤
    regex_items = detect_with_regex_fast(text)
    
    # ì‹¤ëª… ëª©ë¡ íƒì§€
    realname_items = detect_names_with_realname_list(text)
    regex_items.extend(realname_items)
    
    # íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€ (ì¤‘ë³µ ë°©ì§€)
    detected_names = {item['value'] for item in realname_items}
    pattern_items = detect_names_with_patterns(text, detected_names)
    regex_items.extend(pattern_items)
    
    # ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€
    address_items = detect_addresses_smart(text)
    regex_items.extend(address_items)
    
    # 2ì°¨: NER ë³´ê°• (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ)
    try:
        # ë¹„ë™ê¸° NER ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        ner_items = loop.run_until_complete(detect_with_ner_async(text, timeout=0.08))
        loop.close()
    except Exception as e:
        print(f"ğŸ¤– NER ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        ner_items = []
    
    # íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ )
    merged_items = merge_detections_with_priority(regex_items, ner_items)
    
    # ì¹˜í™˜ í† í° í• ë‹¹
    token_map = assign_tokens(merged_items)
    
    print("=" * 60)
    print(f"ğŸ¯ ìµœì¢… íƒì§€ ê²°ê³¼: {len(merged_items)}ê°œ")
    for i, item in enumerate(merged_items, 1):
        token = token_map.get(item['value'], '???')
        print(f"#{i} {item['type']}: '{item['value']}' â†’ {token} (ì‹ ë¢°ë„: {item['confidence']:.2f}, ì¶œì²˜: {item['source']})")
    print("=" * 60)
    
    # í†µê³„ ìƒì„±
    stats = {
        'items_by_type': {},
        'detection_stats': {},
        'total_items': len(merged_items),
        'token_map': token_map
    }
    
    for item in merged_items:
        item_type = item['type']
        source = item['source']
        
        if item_type not in stats['items_by_type']:
            stats['items_by_type'][item_type] = 0
        stats['items_by_type'][item_type] += 1
        
        if source not in stats['detection_stats']:
            stats['detection_stats'][source] = 0
        stats['detection_stats'][source] += 1
    
    return {
        'contains_pii': len(merged_items) > 0,
        'items': merged_items,
        'stats': stats,
        'token_map': token_map
    }

# í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
def detect_with_ner(text: str) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(detect_with_ner_async(text))
        loop.close()
        return result
    except:
        return []

def detect_with_regex(text: str, pools=None) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return detect_with_regex_fast(text)

def detect_names_from_csv(text: str, pools=None) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return detect_names_with_realname_list(text)

def detect_addresses_from_csv(text: str, pools=None) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return detect_addresses_smart(text)

def merge_detections(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return items  # ì´ë¯¸ ë³‘í•©ë¨

# ì›Œí¬í”Œë¡œìš° í•µì‹¬ í•¨ìˆ˜ export
__all__ = [
    'detect_pii_enhanced',
    'detect_with_ner',
    'detect_with_ner_simple', 
    'detect_with_regex',
    'detect_names_from_csv',
    'detect_addresses_from_csv',
    'merge_detections',
    'assign_tokens'  # ì›Œí¬í”Œë¡œìš°ìš©
]