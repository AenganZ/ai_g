# pseudonymization/detection.py
"""
ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ PII íƒì§€ ëª¨ë“ˆ (ê°•í™”ëœ ë²„ì „)
1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤ (90ms ë‚´ì™¸)
2ì°¨: NER ë³´ê°• (íƒ€ì„ì•„ì›ƒ 80ms, ë†’ì€ ì„ê³„ì¹˜)
"""

import re
import asyncio
from typing import List, Dict, Any
from .pools import get_pools

def detect_with_regex_fast(text: str) -> List[Dict[str, Any]]:
    """1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤ (í•µì‹¬ íŒ¨í„´ë§Œ)"""
    
    print("ğŸš€ 1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤")
    
    items = []
    
    # ì´ë©”ì¼ (ê³ ì‹ ë¢°ë„)
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    for match in re.finditer(email_pattern, text):
        items.append({
            "type": "ì´ë©”ì¼",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 0.95,
            "source": "ì •ê·œì‹-ì´ë©”ì¼"
        })
        print(f"ğŸ“§ ì´ë©”ì¼ íƒì§€: '{match.group()}'")
    
    # ì „í™”ë²ˆí˜¸ (í•œêµ­ì‹)
    phone_patterns = [
        r'01[0-9]-\d{4}-\d{4}',  # 010-1234-5678
        r'01[0-9]\d{4}\d{4}',     # 01012345678
        r'\d{2,3}-\d{3,4}-\d{4}', # 02-123-4567, 031-1234-5678
    ]
    
    for pattern in phone_patterns:
        for match in re.finditer(pattern, text):
            items.append({
                "type": "ì „í™”ë²ˆí˜¸",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.9,
                "source": "ì •ê·œì‹-ì „í™”ë²ˆí˜¸"
            })
            print(f"ğŸ“ ì „í™”ë²ˆí˜¸ íƒì§€: '{match.group()}'")
    
    # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ (ë¶€ë¶„ ë§ˆìŠ¤í‚¹ í¬í•¨)
    rrn_patterns = [
        r'\d{6}-[1-4]\d{6}',  # 123456-1234567
        r'\d{6}-[1-4]\*{6}',  # 123456-1******
    ]
    
    for pattern in rrn_patterns:
        for match in re.finditer(pattern, text):
            items.append({
                "type": "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.98,
                "source": "ì •ê·œì‹-ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸"
            })
            print(f"ğŸ†” ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ íƒì§€: '{match.group()}'")
    
    # ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸
    card_pattern = r'\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}'
    for match in re.finditer(card_pattern, text):
        # ê°„ë‹¨í•œ ê²€ì¦ (ì—°ì†ëœ ê°™ì€ ìˆ«ì ì œì™¸)
        card_num = re.sub(r'[- ]', '', match.group())
        if not all(digit == card_num[0] for digit in card_num):
            items.append({
                "type": "ì‹ ìš©ì¹´ë“œ",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.85,
                "source": "ì •ê·œì‹-ì‹ ìš©ì¹´ë“œ"
            })
            print(f"ğŸ’³ ì‹ ìš©ì¹´ë“œ íƒì§€: '{match.group()}'")
    
    print(f"ğŸš€ ê·œì¹™/ì •ê·œì‹ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    
    return items

def detect_names_with_realname_list(text: str) -> List[Dict[str, Any]]:
    """ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ ì´ë¦„ íƒì§€"""
    
    print("ğŸ‘¤ ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ ì´ë¦„ íƒì§€")
    
    items = []
    pools = get_pools()
    
    # ì‹¤ëª… ëª©ë¡ì—ì„œ íƒì§€
    for name in pools.real_names:
        if len(name) >= 2:  # 2ê¸€ì ì´ìƒ
            for match in re.finditer(re.escape(name), text):
                items.append({
                    "type": "ì´ë¦„",
                    "value": name,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95,
                    "source": "ì‹¤ëª…ëª©ë¡"
                })
                print(f"ğŸ‘¤ ì‹¤ëª… íƒì§€: '{name}'")
    
    print(f"ğŸ‘¤ ì‹¤ëª… ëª©ë¡ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    
    return items

def detect_names_with_patterns(text: str, exclude_names: set = None) -> List[Dict[str, Any]]:
    """íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€ (ì¤‘ë³µ ë°©ì§€)"""
    
    print("ğŸ” íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€")
    
    items = []
    exclude_names = exclude_names or set()
    pools = get_pools()
    
    # í•œêµ­ì–´ ì´ë¦„ íŒ¨í„´: [ì„±ì”¨][ì´ë¦„] (2-4ê¸€ì)
    korean_name_pattern = r'[ê°€-í£]{2,4}(?=\s|ë‹˜|ì”¨|ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—ê²Œ|ê»˜ì„œ|ì™€|ê³¼|ì˜|ë¡œ|ìœ¼ë¡œ|$|[^\ê°€-í£])'
    
    for match in re.finditer(korean_name_pattern, text):
        name = match.group()
        
        # ì´ë¯¸ íƒì§€ëœ ì´ë¦„ ì œì™¸
        if name in exclude_names:
            continue
        
        # ì œì™¸ ë‹¨ì–´ í™•ì¸
        if name in pools.name_exclude_words:
            print(f"ğŸš« ì œì™¸ ë‹¨ì–´ ë¬´ì‹œ: '{name}'")
            continue
        
        # ì„±ì”¨ íŒ¨í„´ í™•ì¸
        if name[0] in pools.compound_surnames or name[0] in pools.single_surnames:
            items.append({
                "type": "ì´ë¦„",
                "value": name,
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.8,
                "source": "íŒ¨í„´-ì´ë¦„"
            })
            print(f"ğŸ” íŒ¨í„´ ì´ë¦„ íƒì§€: '{name}'")
    
    print(f"ğŸ” íŒ¨í„´ ì´ë¦„ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    
    return items

def detect_addresses_smart(text: str) -> List[Dict[str, Any]]:
    """ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€ (ì²« ë²ˆì§¸ ì£¼ì†Œë§Œ ì„ íƒ)"""
    
    print("ğŸ  ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€")
    
    items = []
    pools = get_pools()
    detected_locations = []
    
    # ì‹œ/ë„ íƒì§€
    provinces = pools.provinces
    for province in provinces:
        if province in text:
            for match in re.finditer(re.escape(province), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ",
                    "value": province,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.9,
                    "source": "íŒ¨í„´-ì£¼ì†Œ",
                    "location_type": "province"
                })
                print(f"ğŸ—ºï¸ ì‹œ/ë„ íƒì§€: '{province}'")
    
    # ë„ì‹œ íƒì§€
    cities = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…"]
    for city in cities:
        if city in text:
            for match in re.finditer(re.escape(city), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ",
                    "value": city,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.85,
                    "source": "íŒ¨í„´-ì£¼ì†Œ",
                    "location_type": "city"
                })
                print(f"ğŸ™ï¸ ë„ì‹œ íƒì§€: '{city}'")
    
    # êµ¬ íƒì§€ (ëŒ€êµ¬ ë“± íŠ¹ë³„ ì²˜ë¦¬)
    districts = pools.districts
    for district in districts:
        if district in text:
            # "ëŒ€êµ¬"ê°€ í¬í•¨ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if "ëŒ€êµ¬" in district and district != "ëŒ€êµ¬":
                continue  # "ëŒ€êµ¬"ëŠ” ì‹œì´ë¯€ë¡œ êµ¬ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                
            for match in re.finditer(re.escape(district), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ", 
                    "value": district,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.8,
                    "source": "íŒ¨í„´-ì£¼ì†Œ",
                    "location_type": "district"
                })
                print(f"ğŸ˜ï¸ êµ¬ íƒì§€: '{district}'")
    
    # ì¤‘ë³µ ì œê±°: ê²¹ì¹˜ëŠ” ìœ„ì¹˜ì˜ ì£¼ì†Œë“¤ ì¤‘ ì²« ë²ˆì§¸ë§Œ ì„ íƒ
    if detected_locations:
        # ì‹œì‘ ìœ„ì¹˜ë¡œ ì •ë ¬
        detected_locations.sort(key=lambda x: x['start'])
        
        # ì²« ë²ˆì§¸ ì£¼ì†Œë§Œ ì„ íƒ
        first_location = detected_locations[0]
        items.append(first_location)
        
        print(f"ğŸ  ì£¼ì†Œ ì¤‘ë³µ ì œê±°: {len(detected_locations)}ê°œ â†’ 1ê°œ")
        print(f"ğŸ  ì„ íƒëœ ì£¼ì†Œ: '{first_location['value']}'")
    
    return items

async def detect_with_ner_async(text: str, timeout: float = 0.1) -> List[Dict[str, Any]]:
    """2ì°¨: NER ë³´ê°• (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ, ë†’ì€ ì„ê³„ì¹˜)"""
    items = []
    
    try:
        print(f"ğŸ¤– 2ì°¨: NER ë³´ê°• (íƒ€ì„ì•„ì›ƒ: {timeout*1000:.0f}ms)")
        
        # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ NER ì‹¤í–‰
        async def run_ner():
            try:
                from .model import extract_entities_with_ner, is_ner_loaded
                
                if is_ner_loaded():
                    ner_items = extract_entities_with_ner(text)
                    
                    # ë†’ì€ ì„ê³„ì¹˜ ì ìš© (0.9 ì´ìƒ)
                    high_confidence_items = []
                    for item in ner_items:
                        if item['confidence'] > 0.9:
                            item['source'] = 'NER-ê³ ì‹ ë¢°ë„'
                            high_confidence_items.append(item)
                            print(f"ğŸ¤– NER ê³ ì‹ ë¢°ë„ íƒì§€: {item['type']} = '{item['value']}' (ì‹ ë¢°ë„: {item['confidence']:.3f})")
                    
                    return high_confidence_items
                else:
                    print("ğŸ¤– NER ëª¨ë¸ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                    return []
                    
            except Exception as e:
                print(f"ğŸ¤– NER ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return []
        
        # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì‹¤í–‰
        items = await asyncio.wait_for(run_ner(), timeout=timeout)
        print(f"ğŸ¤– 2ì°¨ NER ë³´ê°• ì™„ë£Œ: {len(items)}ê°œ íƒì§€")
        
    except asyncio.TimeoutError:
        print(f"ğŸ¤– NER íƒ€ì„ì•„ì›ƒ ({timeout*1000:.0f}ms) - ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ ì‚¬ìš©")
    except Exception as e:
        print(f"ğŸ¤– NER ë³´ê°• ì‹¤íŒ¨: {e}")
    
    return items

def merge_detections_with_priority(regex_items: List[Dict[str, Any]], 
                                  ner_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ , ìŠ¤íŒ¬ ì¶©ëŒ í•´ê²°)"""
    
    print("ğŸ”„ íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ )")
    
    # ê·œì¹™ ê¸°ë°˜ ê²°ê³¼ê°€ ìš°ì„ 
    merged_items = regex_items.copy()
    
    # NER ê²°ê³¼ ì¶”ê°€ (ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
    for ner_item in ner_items:
        overlapped = False
        
        for regex_item in regex_items:
            # ìŠ¤íŒ¬ ì¶©ëŒ í™•ì¸
            if (ner_item['start'] < regex_item['end'] and 
                ner_item['end'] > regex_item['start']):
                overlapped = True
                print(f"ğŸ”„ ìŠ¤íŒ¬ ì¶©ëŒ ë¬´ì‹œ: NER '{ner_item['value']}' vs ê·œì¹™ '{regex_item['value']}'")
                break
        
        if not overlapped:
            merged_items.append(ner_item)
            print(f"ğŸ”„ NER ê²°ê³¼ ì¶”ê°€: '{ner_item['value']}'")
    
    # ì‹œì‘ ìœ„ì¹˜ë¡œ ì •ë ¬
    merged_items.sort(key=lambda x: x['start'])
    
    print(f"ğŸ”„ ë³‘í•© ì™„ë£Œ: ê·œì¹™ {len(regex_items)}ê°œ + NER {len(ner_items)}ê°œ â†’ {len(merged_items)}ê°œ")
    
    return merged_items

def assign_tokens(items: List[Dict[str, Any]]) -> Dict[str, str]:
    """ì¹˜í™˜ í† í° í• ë‹¹ ([PER_0], [ORG_0], [LOC_0] ë“±)"""
    
    print("ğŸ·ï¸ ì¹˜í™˜ í† í° í• ë‹¹")
    
    # íƒ€ì…ë³„ ì¹´ìš´í„°
    type_counters = {}
    token_map = {}
    
    # íƒ€ì…ë³„ í† í° ì ‘ë‘ì‚¬
    type_prefixes = {
        'ì´ë¦„': 'PER',
        'íšŒì‚¬': 'ORG', 
        'ì£¼ì†Œ': 'LOC',
        'ì´ë©”ì¼': 'EMAIL',
        'ì „í™”ë²ˆí˜¸': 'PHONE',
        'ë‚˜ì´': 'AGE',
        'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸': 'RRN',
        'ì‹ ìš©ì¹´ë“œ': 'CARD',
        'ê³„ì¢Œë²ˆí˜¸': 'ACCT'
    }
    
    for item in items:
        pii_type = item['type']
        pii_value = item['value']
        
        # íƒ€ì…ë³„ ì¹´ìš´í„° ì¦ê°€
        if pii_type not in type_counters:
            type_counters[pii_type] = 0
        
        # í† í° ìƒì„±
        prefix = type_prefixes.get(pii_type, 'MISC')
        token = f"[{prefix}_{type_counters[pii_type]}]"
        
        token_map[pii_value] = token
        type_counters[pii_type] += 1
        
        print(f"ğŸ·ï¸ {pii_value} â†’ {token}")
    
    print(f"ğŸ·ï¸ í† í° í• ë‹¹ ì™„ë£Œ: {len(token_map)}ê°œ")
    
    return token_map

def detect_pii_enhanced(text: str) -> Dict[str, Any]:
    """ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ê°•í™”ëœ PII íƒì§€"""
    
    print("=" * 60)
    print("ğŸ” ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ PII íƒì§€ ì‹œì‘")
    print("=" * 60)
    
    # 1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤
    regex_items = detect_with_regex_fast(text)
    
    # ì‹¤ëª… ëª©ë¡ íƒì§€
    realname_items = detect_names_with_realname_list(text)
    regex_items.extend(realname_items)
    
    # íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€ (ì¤‘ë³µ ë°©ì§€)
    detected_names = {item['value'] for item in realname_items}
    pattern_items = detect_names_with_patterns(text, detected_names)
    regex_items.extend(pattern_items)
    
    # ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€
    address_items = detect_addresses_smart(text)
    regex_items.extend(address_items)
    
    # 2ì°¨: NER ë³´ê°• (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ)
    try:
        # ë¹„ë™ê¸° NER ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        ner_items = loop.run_until_complete(detect_with_ner_async(text, timeout=0.08))
        loop.close()
    except Exception as e:
        print(f"ğŸ¤– NER ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        ner_items = []
    
    # íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ )
    merged_items = merge_detections_with_priority(regex_items, ner_items)
    
    # ì¹˜í™˜ í† í° í• ë‹¹
    token_map = assign_tokens(merged_items)
    
    print("=" * 60)
    print(f"ğŸ¯ ìµœì¢… íƒì§€ ê²°ê³¼: {len(merged_items)}ê°œ")
    for i, item in enumerate(merged_items, 1):
        token = token_map.get(item['value'], '???')
        print(f"#{i} {item['type']}: '{item['value']}' â†’ {token} (ì‹ ë¢°ë„: {item['confidence']:.2f}, ì¶œì²˜: {item['source']})")
    print("=" * 60)
    
    # í†µê³„ ìƒì„±
    stats = {
        'items_by_type': {},
        'detection_stats': {},
        'total_items': len(merged_items),
        'token_map': token_map
    }
    
    for item in merged_items:
        item_type = item['type']
        source = item['source']
        
        if item_type not in stats['items_by_type']:
            stats['items_by_type'][item_type] = 0
        stats['items_by_type'][item_type] += 1
        
        if source not in stats['detection_stats']:
            stats['detection_stats'][source] = 0
        stats['detection_stats'][source] += 1
    
    return {
        'contains_pii': len(merged_items) > 0,
        'items': merged_items,
        'stats': stats,
        'token_map': token_map
    }

# í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
def detect_with_ner(text: str) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(detect_with_ner_async(text))
        loop.close()
        return result
    except:
        return []

def detect_with_ner_simple(text: str) -> List[Dict[str, Any]]:
    """ê°„ì†Œí™”ëœ NER íƒì§€ (ëˆ„ë½ëœ í•¨ìˆ˜ ì¶”ê°€)"""
    print("ğŸ¤– ê°„ì†Œí™”ëœ NER íƒì§€")
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ë™ê¸°ì  NER íƒì§€ ì‹œë„
    try:
        from .model import extract_entities_with_ner, is_ner_loaded
        
        if is_ner_loaded():
            entities = extract_entities_with_ner(text)
            print(f"ğŸ¤– NER ê°„ì†Œ íƒì§€ ì™„ë£Œ: {len(entities)}ê°œ")
            return entities
        else:
            print("ğŸ¤– NER ëª¨ë¸ ë¡œë“œë˜ì§€ ì•ŠìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return []
            
    except Exception as e:
        print(f"ğŸ¤– NER ê°„ì†Œ íƒì§€ ì‹¤íŒ¨: {e}")
        return []

def detect_with_regex(text: str, pools=None) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return detect_with_regex_fast(text)

def detect_names_from_csv(text: str, pools=None) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return detect_names_with_realname_list(text)

def detect_addresses_from_csv(text: str, pools=None) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return detect_addresses_smart(text)

def merge_detections(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜"""
    return items  # ì´ë¯¸ ë³‘í•©ë¨

# ì›Œí¬í”Œë¡œìš° í•µì‹¬ í•¨ìˆ˜ export
__all__ = [
    'detect_pii_enhanced',
    'detect_with_ner',
    'detect_with_ner_simple', 
    'detect_with_regex',
    'detect_names_from_csv',
    'detect_addresses_from_csv',
    'merge_detections',
    'assign_tokens'  # ì›Œí¬í”Œë¡œìš°ìš©
]