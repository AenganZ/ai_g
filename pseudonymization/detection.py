# pseudonymization/detection.py
"""
ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ PII íƒì§€ ëª¨ë“ˆ (ê°•í™”ëœ ë²„ì „)
1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤ (90ms ë‚´ì™¸)
2ì°¨: NER ë³´ê°• (íƒ€ì„ì•„ì›ƒ 80ms, ë†’ì€ ì„ê³„ì¹˜)
"""

import re
import asyncio
from typing import List, Dict, Any
from .pools import get_pools

def detect_with_regex_fast(text: str) -> List[Dict[str, Any]]:
    """1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤ (í•µì‹¬ íŒ¨í„´ë§Œ)"""
    
    print("ğŸš€ 1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤")
    
    items = []
    
    # ì´ë©”ì¼ (ê³ ì‹ ë¢°ë„)
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    for match in re.finditer(email_pattern, text):
        items.append({
            "type": "ì´ë©”ì¼",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 0.95,
            "source": "ì •ê·œì‹-ì´ë©”ì¼"
        })
        print(f"ğŸ“§ ì´ë©”ì¼ íƒì§€: '{match.group()}'")
    
    # ì „í™”ë²ˆí˜¸ (í•œêµ­ì‹)
    phone_patterns = [
        r'01[0-9]-\d{4}-\d{4}',  # 010-1234-5678
        r'01[0-9]\d{4}\d{4}',     # 01012345678
        r'\d{2,3}-\d{3,4}-\d{4}', # 02-123-4567, 031-1234-5678
    ]
    
    for pattern in phone_patterns:
        for match in re.finditer(pattern, text):
            items.append({
                "type": "ì „í™”ë²ˆí˜¸",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.9,
                "source": "ì •ê·œì‹-ì „í™”ë²ˆí˜¸"
            })
            print(f"ğŸ“ ì „í™”ë²ˆí˜¸ íƒì§€: '{match.group()}'")
    
    # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ (ë¶€ë¶„ ë§ˆìŠ¤í‚¹ í¬í•¨)
    rrn_patterns = [
        r'\d{6}-[1-4]\d{6}',  # 123456-1234567
        r'\d{6}-[1-4]\*{6}',  # 123456-1******
    ]
    
    for pattern in rrn_patterns:
        for match in re.finditer(pattern, text):
            items.append({
                "type": "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.98,
                "source": "ì •ê·œì‹-ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸"
            })
            print(f"ğŸ†” ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ íƒì§€: '{match.group()}'")
    
    # ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸
    card_pattern = r'\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}'
    for match in re.finditer(card_pattern, text):
        # ê°„ë‹¨í•œ ê²€ì¦ (ì—°ì†ëœ ê°™ì€ ìˆ«ì ì œì™¸)
        card_num = re.sub(r'[- ]', '', match.group())
        if not all(digit == card_num[0] for digit in card_num):
            items.append({
                "type": "ì‹ ìš©ì¹´ë“œ",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.85,
                "source": "ì •ê·œì‹-ì‹ ìš©ì¹´ë“œ"
            })
            print(f"ğŸ’³ ì‹ ìš©ì¹´ë“œ íƒì§€: '{match.group()}'")
    
    print(f"ğŸš€ ê·œì¹™/ì •ê·œì‹ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    
    return items

def detect_names_with_realname_list(text: str) -> List[Dict[str, Any]]:
    """ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ ì´ë¦„ íƒì§€ (ì œì™¸ ë‹¨ì–´ ì²´í¬ ì¶”ê°€)"""
    
    print("ğŸ‘¤ ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ ì´ë¦„ íƒì§€")
    
    items = []
    pools = get_pools()
    
    # ì‹¤ëª… ëª©ë¡ì—ì„œ íƒì§€
    for name in pools.real_names:
        if len(name) >= 2:  # 2ê¸€ì ì´ìƒ
            # ì œì™¸ ë‹¨ì–´ í™•ì¸ (ì¶”ê°€ë¨)
            if name in pools.name_exclude_words:
                print(f"ğŸš« ì‹¤ëª… ëª©ë¡ ì œì™¸ ë‹¨ì–´ ë¬´ì‹œ: '{name}'")
                continue
            
            # ì´ë¦„ìœ¼ë¡œ ë³´ê¸° ì–´ë ¤ìš´ ë‹¨ì–´ë“¤ í•„í„°ë§ (ì¶”ê°€ë¨)
            if _is_invalid_name(name):
                print(f"ğŸš« ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¦„ ë¬´ì‹œ: '{name}'")
                continue
            
            for match in re.finditer(re.escape(name), text):
                items.append({
                    "type": "ì´ë¦„",
                    "value": name,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95,
                    "source": "ì‹¤ëª…ëª©ë¡"
                })
                print(f"ğŸ‘¤ ì‹¤ëª… íƒì§€: '{name}'")
    
    print(f"ğŸ‘¤ ì‹¤ëª… ëª©ë¡ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    
    return items

def _is_invalid_name(name: str) -> bool:
    """ì´ë¦„ìœ¼ë¡œ ë³´ê¸° ì–´ë ¤ìš´ ë‹¨ì–´ì¸ì§€ í™•ì¸"""
    
    # ë¬¸ë²• ìš”ì†Œë“¤
    grammar_words = {
        'ì´ë¦„ì€', 'ì´ë¦„ì´', 'ì´ê³ ', 'ì´ë©°', 'ì´ë‹¤', 'ì…ë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤',
        'í–ˆì–´ìš”', 'í•´ìš”', 'ì´ì—ìš”', 'ì˜ˆìš”', 'ì´ì•¼', 'ì•¼', 'ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ë¡œ',
        'ê·¸ëŸ°', 'ê·¸ë˜', 'ì´ëŸ°', 'ì €ëŸ°', 'ê°™ì€', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ì˜¤ë˜ëœ'
    }
    
    # ì§€ì—­ëª…ë“¤ (ì´ë¦„ì´ ì•„ë‹˜)
    location_words = {
        'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
        'ê°•ë‚¨', 'ê°•ë¶', 'ê°•ì„œ', 'ê°•ë™', 'ì„œì´ˆ', 'ì†¡íŒŒ', 'ë§ˆí¬', 'ìš©ì‚°',
        'ì¤‘êµ¬', 'ë™êµ¬', 'ì„œêµ¬', 'ë‚¨êµ¬', 'ë¶êµ¬', 'ìˆ˜ì›', 'ì„±ë‚¨', 'ì•ˆì–‘'
    }
    
    # ì¼ë°˜ ëª…ì‚¬ë“¤
    common_words = {
        'ì‚¬ëŒ', 'í•™ìƒ', 'ì„ ìƒ', 'ì˜ì‚¬', 'ê°„í˜¸ì‚¬', 'íšŒì‚¬', 'í•™êµ', 'ë³‘ì›',
        'ìŒì‹', 'ìš”ë¦¬', 'ì±…ìƒ', 'ì˜ì', 'ì»´í“¨í„°', 'í•¸ë“œí°', 'ìë™ì°¨', 'ì§‘',
        'ë‚˜ë¬´', 'ê½ƒ', 'ë¬¼', 'ë¶ˆ', 'ë°”ëŒ', 'í•˜ëŠ˜', 'êµ¬ë¦„', 'ë³„'
    }
    
    # ìˆ«ìê°€ í¬í•¨ëœ ê²½ìš°
    if any(char.isdigit() for char in name):
        return True
    
    # íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ê²½ìš°
    if not name.replace(' ', '').isalpha():
        return True
    
    # ë¬¸ë²• ìš”ì†Œ, ì§€ì—­ëª…, ì¼ë°˜ ëª…ì‚¬ ì²´í¬
    if name in grammar_words or name in location_words or name in common_words:
        return True
    
    # 1ê¸€ì ì„±ì”¨ë§Œ ìˆëŠ” ê²½ìš° (ì„±ì”¨ê°€ ì•„ë‹Œ 1ê¸€ìëŠ” ì œì™¸)
    pools = get_pools()
    if len(name) == 1 and name not in pools.single_surnames:
        return True
    
    return False

def detect_names_with_patterns(text: str, exclude_names: set = None) -> List[Dict[str, Any]]:
    """íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€ (ì¤‘ë³µ ë°©ì§€, ì—„ê²©í•œ í•„í„°ë§)"""
    
    print("ğŸ” íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€")
    
    items = []
    exclude_names = exclude_names or set()
    pools = get_pools()
    
    # í•œêµ­ì–´ ì´ë¦„ íŒ¨í„´: [ì„±ì”¨][ì´ë¦„] (2-4ê¸€ì)
    korean_name_pattern = r'[ê°€-í£]{2,4}(?=\s|ë‹˜|ì”¨|ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—ê²Œ|ê»˜ì„œ|ì™€|ê³¼|ì˜|ë¡œ|ìœ¼ë¡œ|$|[^\ê°€-í£])'
    
    for match in re.finditer(korean_name_pattern, text):
        name = match.group()
        
        # ì´ë¯¸ íƒì§€ëœ ì´ë¦„ ì œì™¸
        if name in exclude_names:
            continue
        
        # ì œì™¸ ë‹¨ì–´ í™•ì¸
        if name in pools.name_exclude_words:
            print(f"ğŸš« ì œì™¸ ë‹¨ì–´ ë¬´ì‹œ: '{name}'")
            continue
        
        # ì´ë¦„ìœ¼ë¡œ ë³´ê¸° ì–´ë ¤ìš´ ë‹¨ì–´ë“¤ í•„í„°ë§ (ì¶”ê°€ë¨)
        if _is_invalid_name(name):
            print(f"ğŸš« ìœ íš¨í•˜ì§€ ì•Šì€ íŒ¨í„´ ë¬´ì‹œ: '{name}'")
            continue
        
        # ì„±ì”¨ íŒ¨í„´ í™•ì¸ (ë” ì—„ê²©í•˜ê²Œ)
        if len(name) >= 2 and (name[0] in pools.compound_surnames or name[0] in pools.single_surnames):
            # ì„±ì”¨ + ì´ë¦„ ì¡°í•©ì´ ì‹¤ì œ ì´ë¦„ì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ í™•ì¸
            if _looks_like_real_name(name):
                items.append({
                    "type": "ì´ë¦„",
                    "value": name,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.8,
                    "source": "íŒ¨í„´-ì´ë¦„"
                })
                print(f"ğŸ” íŒ¨í„´ ì´ë¦„ íƒì§€: '{name}'")
            else:
                print(f"ğŸš« ì‹¤ì œ ì´ë¦„ì´ ì•„ë‹Œ íŒ¨í„´ ë¬´ì‹œ: '{name}'")
    
    print(f"ğŸ” íŒ¨í„´ ì´ë¦„ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    
    return items

def _looks_like_real_name(name: str) -> bool:
    """ì‹¤ì œ ì´ë¦„ì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ í™•ì¸"""
    
    # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²½ìš°
    if len(name) < 2 or len(name) > 4:
        return False
    
    # ê°™ì€ ê¸€ì ë°˜ë³µ (ì˜ˆ: "ê°€ê°€", "ë‚˜ë‚˜ë‚˜")
    if len(set(name)) == 1:
        return False
    
    # ëª…ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš°ë“¤
    noun_endings = ['ì‹œì¥', 'ì˜ì›', 'ì‚¬ì¥', 'ë¶€ì¥', 'ê³¼ì¥', 'íŒ€ì¥', 'íšŒì¥', 'ì‚¬ë¬´ì†Œ', 'ë³‘ì›', 'í•™êµ']
    for ending in noun_endings:
        if name.endswith(ending):
            return False
    
    # ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ë“¤
    verb_endings = ['í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'í¬ë‹¤', 'ì‘ë‹¤']
    for ending in verb_endings:
        if name.endswith(ending[:2]):  # ì–´ë¯¸ì˜ ì²˜ìŒ 2ê¸€ìë¡œ ì²´í¬
            return False
    
    return True

def detect_addresses_smart(text: str) -> List[Dict[str, Any]]:
    """ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€ (ì²« ë²ˆì§¸ ì£¼ì†Œë§Œ ì„ íƒ)"""
    
    print("ğŸ  ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€")
    
    items = []
    pools = get_pools()
    detected_locations = []
    
    # ì‹œ/ë„ íƒì§€
    provinces = pools.provinces
    for province in provinces:
        if province in text:
            for match in re.finditer(re.escape(province), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ",
                    "value": province,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.9,
                    "source": "íŒ¨í„´-ì£¼ì†Œ"
                })
                print(f"ğŸ—ºï¸ ì‹œ/ë„ íƒì§€: '{province}'")
    
    # êµ¬/êµ° íƒì§€
    districts = pools.districts
    for district in districts:
        if district in text:
            for match in re.finditer(re.escape(district), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ",
                    "value": district,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.85,
                    "source": "íŒ¨í„´-ì£¼ì†Œ"
                })
                print(f"ğŸ˜ï¸ êµ¬ íƒì§€: '{district}'")
    
    # ë„ì‹œ íƒì§€
    cities = pools.cities
    for city in cities:
        if city in text:
            for match in re.finditer(re.escape(city), text):
                detected_locations.append({
                    "type": "ì£¼ì†Œ",
                    "value": city,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.85,
                    "source": "íŒ¨í„´-ì£¼ì†Œ"
                })
                print(f"ğŸ™ï¸ ë„ì‹œ íƒì§€: '{city}'")
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    detected_locations.sort(key=lambda x: x["start"])
    
    # ì¤‘ë³µ ìœ„ì¹˜ ì œê±°
    unique_locations = []
    used_positions = set()
    
    for location in detected_locations:
        position_key = (location["start"], location["end"])
        if position_key not in used_positions:
            unique_locations.append(location)
            used_positions.add(position_key)
    
    print(f"ğŸ  ì£¼ì†Œ ì¤‘ë³µ ì œê±°: {len(detected_locations)}ê°œ â†’ {len(unique_locations)}ê°œ")
    
    # ì²« ë²ˆì§¸ ì£¼ì†Œë§Œ ì„ íƒ
    if unique_locations:
        selected = unique_locations[0]
        items.append(selected)
        print(f"ğŸ  ì„ íƒëœ ì£¼ì†Œ: '{selected['value']}'")
    
    return items

# NER ê´€ë ¨ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
async def detect_with_ner_async(text: str, timeout: float = 0.08) -> List[Dict[str, Any]]:
    """ë¹„ë™ê¸° NER íƒì§€ (íƒ€ì„ì•„ì›ƒ ì ìš©)"""
    
    print(f"ğŸ¤– 2ì°¨: NER ë³´ê°• (íƒ€ì„ì•„ì›ƒ: {int(timeout*1000)}ms)")
    
    try:
        # íƒ€ì„ì•„ì›ƒ ì ìš©
        ner_task = asyncio.create_task(_run_ner_detection(text))
        ner_items = await asyncio.wait_for(ner_task, timeout=timeout)
        
        print(f"ğŸ¤– 2ì°¨ NER ë³´ê°• ì™„ë£Œ: {len(ner_items)}ê°œ íƒì§€")
        return ner_items
        
    except asyncio.TimeoutError:
        print(f"ğŸ¤– NER íƒ€ì„ì•„ì›ƒ ({int(timeout*1000)}ms) - ì •ê·œì‹ë§Œ ì‚¬ìš©")
        return []
    except Exception as e:
        print(f"ğŸ¤– NER ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return []

async def _run_ner_detection(text: str) -> List[Dict[str, Any]]:
    """NER ëª¨ë¸ ì‹¤í–‰"""
    from .model import get_ner_model, is_ner_loaded
    
    if not is_ner_loaded():
        return []
    
    try:
        ner_model = get_ner_model()
        entities = ner_model.extract_entities(text)
        
        # ë†’ì€ ì„ê³„ì¹˜ ì ìš© (ê°„ì†Œí™” ëª¨ë“œ)
        filtered_entities = []
        for entity in entities:
            if entity.get('confidence', 0) >= 0.8:  # ë†’ì€ ì„ê³„ì¹˜
                filtered_entities.append({
                    "type": entity['type'],
                    "value": entity['value'],
                    "start": entity.get('start', 0),
                    "end": entity.get('end', 0),
                    "confidence": entity['confidence'],
                    "source": "NER"
                })
                print(f"ğŸ¤– NER íƒì§€: {entity['type']} = '{entity['value']}' (ì‹ ë¢°ë„: {entity['confidence']:.2f})")
        
        return filtered_entities
        
    except Exception as e:
        print(f"ğŸ¤– NER ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return []

def merge_detections_with_priority(regex_items: List[Dict[str, Any]], ner_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ , ì¤‘ë³µ ì œê±°)"""
    
    print("ğŸ”„ íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ )")
    
    # ìœ„ì¹˜ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
    merged_items = []
    used_positions = set()
    
    # 1. ê·œì¹™/ì •ê·œì‹ ê²°ê³¼ ë¨¼ì € ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    for item in regex_items:
        start, end = item['start'], item['end']
        position_key = (start, end, item['value'])
        
        if position_key not in used_positions:
            merged_items.append(item)
            used_positions.add(position_key)
    
    # 2. NER ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µë˜ì§€ ì•Šì€ ê²ƒë§Œ)
    for item in ner_items:
        start, end = item['start'], item['end']
        position_key = (start, end, item['value'])
        
        # ê²¹ì¹˜ëŠ” ìœ„ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        overlapping = False
        for used_start, used_end, used_value in used_positions:
            if not (end <= used_start or start >= used_end):  # ê²¹ì¹¨ ì²´í¬
                overlapping = True
                break
        
        if not overlapping:
            merged_items.append(item)
            used_positions.add(position_key)
    
    print(f"ğŸ”„ ë³‘í•© ì™„ë£Œ: ê·œì¹™ {len(regex_items)}ê°œ + NER {len(ner_items)}ê°œ â†’ {len(merged_items)}ê°œ")
    
    return merged_items

def assign_tokens(items: List[Dict[str, Any]]) -> Dict[str, str]:
    """ì¹˜í™˜ í† í° í• ë‹¹ ([PER_0], [ORG_0], [LOC_0] ë“±)"""
    
    print("ğŸ·ï¸ ì¹˜í™˜ í† í° í• ë‹¹")
    
    # íƒ€ì…ë³„ ì¹´ìš´í„°
    type_counters = {}
    token_map = {}
    
    # íƒ€ì…ë³„ í† í° ì ‘ë‘ì‚¬
    type_prefixes = {
        'ì´ë¦„': 'PER',
        'íšŒì‚¬': 'ORG', 
        'ì£¼ì†Œ': 'LOC',
        'ì´ë©”ì¼': 'EMAIL',
        'ì „í™”ë²ˆí˜¸': 'PHONE',
        'ë‚˜ì´': 'AGE',
        'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸': 'RRN',
        'ì‹ ìš©ì¹´ë“œ': 'CARD',
        'ê³„ì¢Œë²ˆí˜¸': 'ACCT'
    }
    
    for item in items:
        pii_type = item['type']
        pii_value = item['value']
        
        # íƒ€ì…ë³„ ì¹´ìš´í„° ì¦ê°€
        if pii_type not in type_counters:
            type_counters[pii_type] = 0
        
        # í† í° ìƒì„±
        prefix = type_prefixes.get(pii_type, 'MISC')
        token = f"[{prefix}_{type_counters[pii_type]}]"
        
        token_map[pii_value] = token
        type_counters[pii_type] += 1
        
        print(f"ğŸ·ï¸ {pii_value} â†’ {token}")
    
    print(f"ğŸ·ï¸ í† í° í• ë‹¹ ì™„ë£Œ: {len(token_map)}ê°œ")
    
    return token_map

def detect_pii_enhanced(text: str) -> Dict[str, Any]:
    """ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ê°•í™”ëœ PII íƒì§€"""
    
    print("=" * 60)
    print("ğŸ” ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ PII íƒì§€ ì‹œì‘")
    print("=" * 60)
    
    # 1ì°¨: ê·œì¹™/ì •ê·œì‹ ê³ ì† íŒ¨ìŠ¤
    regex_items = detect_with_regex_fast(text)
    
    # ì‹¤ëª… ëª©ë¡ íƒì§€ (ì œì™¸ ë‹¨ì–´ ì²´í¬ ì¶”ê°€ë¨)
    realname_items = detect_names_with_realname_list(text)
    regex_items.extend(realname_items)
    
    # íŒ¨í„´ ê¸°ë°˜ ì´ë¦„ íƒì§€ (ì¤‘ë³µ ë°©ì§€, ì—„ê²©í•œ í•„í„°ë§)
    detected_names = {item['value'] for item in realname_items}
    pattern_items = detect_names_with_patterns(text, detected_names)
    regex_items.extend(pattern_items)
    
    # ìŠ¤ë§ˆíŠ¸ ì£¼ì†Œ íƒì§€
    address_items = detect_addresses_smart(text)
    regex_items.extend(address_items)
    
    # 2ì°¨: NER ë³´ê°• (ë¹„ë™ê¸°, íƒ€ì„ì•„ì›ƒ)
    try:
        # ë¹„ë™ê¸° NER ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        ner_items = loop.run_until_complete(detect_with_ner_async(text, timeout=0.08))
        loop.close()
    except Exception as e:
        print(f"ğŸ¤– NER ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        ner_items = []
    
    # íƒì§€ ê²°ê³¼ ë³‘í•© (ê·œì¹™ ìš°ì„ )
    merged_items = merge_detections_with_priority(regex_items, ner_items)
    
    # ì¹˜í™˜ í† í° í• ë‹¹
    token_map = assign_tokens(merged_items)
    
    print("=" * 60)
    print(f"ğŸ¯ ìµœì¢… íƒì§€ ê²°ê³¼: {len(merged_items)}ê°œ")
    for i, item in enumerate(merged_items, 1):
        token = token_map.get(item['value'], '???')
        print(f"#{i} {item['type']}: '{item['value']}' â†’ {token} (ì‹ ë¢°ë„: {item['confidence']:.2f}, ì¶œì²˜: {item['source']})")
    print("=" * 60)
    
    # í†µê³„ ìƒì„±
    stats = {
        "detection_time": 0,  # í˜¸ì¶œí•˜ëŠ” ê³³ì—ì„œ ì„¤ì •
        "items_by_type": {},
        "detection_stats": {
            "regex_items": len(regex_items) - len(address_items),
            "ner_items": len(ner_items),
            "total_items": len(merged_items)
        },
        "token_map": token_map
    }
    
    # íƒ€ì…ë³„ í†µê³„
    for item in merged_items:
        pii_type = item['type']
        if pii_type not in stats['items_by_type']:
            stats['items_by_type'][pii_type] = 0
        stats['items_by_type'][pii_type] += 1
    
    return {
        "items": merged_items,
        "stats": stats
    }

# ê¸°ì¡´ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
def detect_with_ner(text: str) -> List[Dict[str, Any]]:
    """NER íƒì§€ (í˜¸í™˜ì„±)"""
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(detect_with_ner_async(text))
        loop.close()
        return result
    except:
        return []

def detect_with_ner_simple(text: str) -> List[Dict[str, Any]]:
    """ê°„ì†Œí™”ëœ NER íƒì§€ (í˜¸í™˜ì„±)"""
    return detect_with_ner(text)

def detect_with_regex(text: str) -> List[Dict[str, Any]]:
    """ì •ê·œì‹ íƒì§€ (í˜¸í™˜ì„±)"""
    return detect_with_regex_fast(text)

def detect_names_from_csv(text: str) -> List[Dict[str, Any]]:
    """CSV ì´ë¦„ íƒì§€ (í˜¸í™˜ì„±)"""
    return detect_names_with_realname_list(text)

def detect_addresses_from_csv(text: str) -> List[Dict[str, Any]]:
    """CSV ì£¼ì†Œ íƒì§€ (í˜¸í™˜ì„±)"""
    return detect_addresses_smart(text)

def merge_detections(items1: List[Dict[str, Any]], items2: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """íƒì§€ ê²°ê³¼ ë³‘í•© (í˜¸í™˜ì„±)"""
    return merge_detections_with_priority(items1, items2)