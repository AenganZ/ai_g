# Manager
from typing import Dict, Any, Optional
from .model import load_model
from .core import pseudonymize_text

class PseudonymizationManager:
    """ê°€ëª…í™” ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤‘ì•™ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = None
        self._initialized = False
    
    def initialize(self) -> None:
        """ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤"""
        if self._initialized:
            print("âš ï¸  ëª¨ë¸ì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
            
        print("ğŸ¤– Qwen ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model, self.tokenizer, self.device = load_model()
        self._initialized = True
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! Device: {self.device}")
    
    def is_initialized(self) -> bool:
        """ëª¨ë¸ ì´ˆê¸°í™” ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤"""
        return self._initialized
    
    def pseudonymize(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ë¥¼ ê°€ëª…í™”í•©ë‹ˆë‹¤
        
        Args:
            text: ê°€ëª…í™”í•  ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            Dict containing:
                - masked_prompt: ê°€ëª…í™”ëœ í…ìŠ¤íŠ¸
                - detection: íƒì§€ëœ ê°œì¸ì •ë³´ ì •ë³´
                
        Raises:
            RuntimeError: ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        """
        if not self._initialized:
            raise RuntimeError("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        return pseudonymize_text(text, self.model, self.tokenizer, self.device)
    
    def get_device_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
        if not self._initialized:
            return {"status": "not_initialized"}
        
        import torch
        gpu_info = {}
        
        if self.device == "cuda" and torch.cuda.is_available():
            gpu_info = {
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB",
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1e9:.1f}GB",
                "gpu_memory_cached": f"{torch.cuda.memory_reserved(0) / 1e9:.1f}GB"
            }
        
        return {
            "status": "initialized",
            "device": self.device,
            "gpu_info": gpu_info
        }

# singleton instance
_manager_instance: Optional[PseudonymizationManager] = None

def get_manager() -> PseudonymizationManager:
    """ì „ì—­ PseudonymizationManager ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
    global _manager_instance
    if _manager_instance is None:
        _manager_instance = PseudonymizationManager()
    return _manager_instance
