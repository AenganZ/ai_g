# pseudonymization/model.py - ê°œì„ ëœ NER ëª¨ë¸
import os
import torch
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

# NER ëª¨ë¸ ê´€ë ¨ import
try:
    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
    NER_AVAILABLE = True
except ImportError:
    NER_AVAILABLE = False
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install transformers torchë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# ì‹¤ì œë¡œ ì¡´ì¬í•˜ê³  ê²€ì¦ëœ í•œêµ­ì–´ NER ëª¨ë¸ë“¤ (HuggingFaceì—ì„œ í™•ì¸ë¨)
NER_MODELS = [
    "monologg/koelectra-base-v3-naver-ner",             # 1ìˆœìœ„: ì‹¤ì œ ì¡´ì¬, ë‹¤ìš´ë¡œë“œ í™•ì¸ë¨
    "klue/roberta-large-ner",                           # 2ìˆœìœ„: KLUE ê³µì‹ (ì¡´ì¬ í™•ì¸ í•„ìš”)
    "Leo97/KoELECTRA-small-v3-modu-ner",               # 3ìˆœìœ„: ë‹¤ìš´ë¡œë“œ í™•ì¸ë¨
    "monologg/kobert-ner"                               # 4ìˆœìœ„: KoBERT ê¸°ë°˜ (í´ë°±ìš©)
]

class ImprovedNERModel:
    """ê°œì„ ëœ NER ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.device = self._get_device()
        self.loaded = False
        self.model_name = None
    
    def _get_device(self):
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if torch.cuda.is_available():
            return 0  # GPU
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"  # Apple Silicon
        else:
            return -1  # CPU
    
    def load_model(self) -> bool:
        """ì•ˆì •ì ì¸ NER ëª¨ë¸ ë¡œë“œ (íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ í•´ê²°)"""
        if not NER_AVAILABLE:
            print("âŒ NER ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return False
        
        for model_name in NER_MODELS:
            try:
                print(f"ğŸ”„ NER ëª¨ë¸ ë¡œë”© ì‹œë„: {model_name}")
                
                # í† í¬ë‚˜ì´ì € ë¡œë“œ (ì•ˆì „í•œ ì„¤ì •)
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    use_fast=True
                )
                
                # ëª¨ë¸ ë¡œë“œ
                self.model = AutoModelForTokenClassification.from_pretrained(model_name)
                
                # íŒŒì´í”„ë¼ì¸ ìƒì„± (ì˜ëª»ëœ ë§¤ê°œë³€ìˆ˜ ì œê±°)
                self.pipeline = pipeline(
                    "ner", 
                    model=self.model, 
                    tokenizer=self.tokenizer,
                    aggregation_strategy="simple",
                    device=self.device
                    # max_length, truncation ë“±ì€ ì—¬ê¸°ì„œ ì œê±° (ì¶”ë¡  ì‹œ ì‚¬ìš©)
                )
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ ê²€ì¦
                test_result = self.pipeline("ê¹€ì² ìˆ˜ëŠ” ì„œìš¸ì— ì‚´ê³  ìˆìŠµë‹ˆë‹¤.")
                
                self.loaded = True
                self.model_name = model_name
                print(f"âœ… NER ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                print(f"   ğŸ“± ë””ë°”ì´ìŠ¤: {self.device}")
                print(f"   ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(test_result)}ê°œ ì—”í‹°í‹° íƒì§€")
                
                # ì²« ë²ˆì§¸ë¡œ ì„±ê³µí•œ ëª¨ë¸ ì‚¬ìš©
                return True
                
            except Exception as e:
                print(f"âŒ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
                # êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ìœ í˜• í™•ì¸
                error_msg = str(e).lower()
                if "max_length" in error_msg or "unexpected keyword" in error_msg:
                    print(f"   ğŸ”§ íŒŒì´í”„ë¼ì¸ ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜ - ë‹¤ìŒ ëª¨ë¸ ì‹œë„")
                elif "not a valid model" in error_msg:
                    print(f"   ğŸ”§ ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ë‹¤ìŒ ëª¨ë¸ ì‹œë„")
                continue
        
        print("âŒ ëª¨ë“  NER ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        print("ğŸ’¡ ë‹¤ìŒ í•´ê²° ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”:")
        print("   1. pip install transformers torch --upgrade")
        print("   2. ìºì‹œ ì‚­ì œ: rm -rf ~/.cache/huggingface/ (Windows: rmdir /s %USERPROFILE%\\.cache\\huggingface)")
        print("   3. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        self.loaded = False
        return False
    
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return self.loaded
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ (ì•ˆì •ì„± ê°œì„ )"""
        if not self.loaded or not self.pipeline:
            return []
        
        try:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ëª¨ë¸ ìµœëŒ€ ê¸¸ì´ë¥¼ ê³ ë ¤)
            max_length = 400  # ì•ˆì „í•œ ê¸¸ì´ë¡œ ì œí•œ
            if len(text) > max_length:
                text = text[:max_length]
                print(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ {max_length}ìë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤.")
            
            # NER íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ)
            ner_results = self.pipeline(
                text,
                aggregation_strategy="simple",
                return_all_scores=False,
                stride=0,
                truncation=True,
                max_length=512
            )
            
            entities = []
            for entity in ner_results:
                entity_type = entity.get('entity_group', entity.get('entity', ''))
                entity_text = entity.get('word', '')
                confidence = entity.get('score', 0.0)
                start = entity.get('start', 0)
                end = entity.get('end', 0)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬ (í† í¬ë‚˜ì´ì € ì•„í‹°íŒ©íŠ¸ ì œê±°)
                entity_text = entity_text.replace('##', '').strip()
                
                # ì‹ ë¢°ë„ ì„ê³„ê°’ (ëª¨ë¸ë³„ ì¡°ì •)
                if 'small' in self.model_name:
                    threshold = 0.7  # ì‘ì€ ëª¨ë¸ì€ ì„ê³„ê°’ ë‚®ì¶¤
                elif 'klue' in self.model_name:
                    threshold = 0.8  # KLUE ëª¨ë¸ì€ ë†’ì€ ì„ê³„ê°’
                else:
                    threshold = 0.75 # ê¸°ë³¸ê°’
                
                if confidence > threshold and len(entity_text) >= 2:
                    pii_type = self._map_ner_label_to_pii_type(entity_type)
                    if pii_type:
                        entities.append({
                            "type": pii_type,
                            "value": entity_text,
                            "start": start,
                            "end": end,
                            "confidence": confidence,
                            "source": f"NER-{self.model_name.split('/')[-1]}"
                        })
            
            print(f"ğŸ¤– NER ({self.model_name.split('/')[-1]}) íƒì§€: {len(entities)}ê°œ")
            return entities
            
        except Exception as e:
            print(f"âŒ NER ê°œì²´ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
            import traceback
            print("ìƒì„¸ ì˜¤ë¥˜:", traceback.format_exc())
            return []
    
    def _map_ner_label_to_pii_type(self, label: str) -> Optional[str]:
        """NER ë¼ë²¨ì„ PII íƒ€ì…ìœ¼ë¡œ ë§¤í•‘ (KLUE ë° ê¸°íƒ€ ëª¨ë¸ ì§€ì›)"""
        # ë¼ë²¨ ì •ê·œí™” (B-, I- ì ‘ë‘ì‚¬ ì œê±°)
        clean_label = label.replace('B-', '').replace('I-', '').upper()
        
        mapping = {
            # í‘œì¤€ ë¼ë²¨ë“¤
            'PER': 'ì´ë¦„',
            'PERSON': 'ì´ë¦„',
            'LOC': 'ì£¼ì†Œ',
            'LOCATION': 'ì£¼ì†Œ',
            'ORG': 'íšŒì‚¬',
            'ORGANIZATION': 'íšŒì‚¬',
            'MISC': 'ê¸°íƒ€',
            
            # KLUE NER ë¼ë²¨ë“¤
            'PS': 'ì´ë¦„',      # Person
            'LC': 'ì£¼ì†Œ',      # Location  
            'OG': 'íšŒì‚¬',      # Organization
            'DT': 'ê¸°íƒ€',      # Date
            'TI': 'ê¸°íƒ€',      # Time
            'QT': 'ê¸°íƒ€',      # Quantity
            
            # KoELECTRA ë“± ê¸°íƒ€ ëª¨ë¸ ë¼ë²¨ë“¤
            'PRS': 'ì´ë¦„',     # Person
            'LOC': 'ì£¼ì†Œ',     # Location
            'ORG': 'íšŒì‚¬',     # Organization
            'NUM': 'ê¸°íƒ€',     # Number
            'DATE': 'ê¸°íƒ€',    # Date
            'TIME': 'ê¸°íƒ€',    # Time
            
            # ì¶”ê°€ ë§¤í•‘
            'GPE': 'ì£¼ì†Œ',     # Geopolitical entity
            'FAC': 'ì£¼ì†Œ',     # Facility
            'NORP': 'ê¸°íƒ€',    # Nationalities, religious, political groups
        }
        
        result = mapping.get(clean_label)
        if result:
            print(f"   ë§¤í•‘: {label} -> {result}")
        return result

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
_ner_model_instance = None

def get_ner_model() -> ImprovedNERModel:
    """NER ëª¨ë¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ner_model_instance
    
    if _ner_model_instance is None:
        _ner_model_instance = ImprovedNERModel()
    
    return _ner_model_instance

def load_ner_model() -> bool:
    """NER ëª¨ë¸ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ê°€ëŠ¥)"""
    model = get_ner_model()
    return model.load_model()

def extract_entities_with_ner(text: str) -> List[Dict[str, Any]]:
    """NER ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°œì²´ëª… ì¶”ì¶œ"""
    model = get_ner_model()
    
    if not model.is_loaded():
        print("âš ï¸ NER ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    return model.extract_entities(text)

def is_ner_available() -> bool:
    """NER ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    return NER_AVAILABLE

def is_ner_loaded() -> bool:
    """NER ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
    model = get_ner_model()
    return model.is_loaded()

def get_model_info() -> Dict[str, Any]:
    """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    model = get_ner_model()
    return {
        "loaded": model.is_loaded(),
        "model_name": model.model_name if model.is_loaded() else None,
        "device": model.device,
        "available_models": NER_MODELS
    }

# í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ë“¤
def call_qwen_detect_pii(original_prompt: str, model=None, tokenizer=None, device=None) -> Dict[str, Any]:
    """ê¸°ì¡´ í˜¸í™˜ì„ ìœ„í•œ í•¨ìˆ˜ (ê°œì„ ëœ NERë¡œ ë³€ê²½)"""
    print("âš ï¸ call_qwen_detect_piiëŠ” deprecatedì…ë‹ˆë‹¤. extract_entities_with_nerë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    entities = extract_entities_with_ner(original_prompt)
    
    return {
        "items": entities,
        "contains_pii": len(entities) > 0
    }

def pick_device_and_dtype():
    """ë””ë°”ì´ìŠ¤ ë° ë°ì´í„° íƒ€ì… ì„ íƒ (í˜¸í™˜ì„±)"""
    model = get_ner_model()
    device = model.device
    
    if device == 0:  # GPU
        return "cuda", torch.bfloat16
    elif device == "mps":  # Apple Silicon
        return "mps", torch.float16
    else:  # CPU
        return "cpu", torch.float32

def load_model():
    """ëª¨ë¸ ë¡œë“œ (í˜¸í™˜ì„±)"""
    return load_ner_model()

# ëª¨ë“ˆ ì´ˆê¸°í™” ì‹œ ì •ë³´ ì¶œë ¥
if __name__ == "__main__":
    print("ğŸ­ ê°œì„ ëœ NER ëª¨ë¸ ëª¨ë“ˆ")
    print(f"ğŸ“± Transformers ì‚¬ìš© ê°€ëŠ¥: {NER_AVAILABLE}")
    print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
    for i, model in enumerate(NER_MODELS, 1):
        print(f"   {i}. {model}")
    
    if NER_AVAILABLE:
        model = get_ner_model()
        success = model.load_model()
        if success:
            # í…ŒìŠ¤íŠ¸
            test_text = "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ê¹€í…ŒìŠ¤íŠ¸ì´ê³  ì„œìš¸ ê°•ë‚¨êµ¬ì— ì‚´ê³  ìˆìŠµë‹ˆë‹¤."
            entities = model.extract_entities(test_text)
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(entities)}ê°œ ê°œì²´ íƒì§€")
            for entity in entities:
                print(f"   {entity['type']}: {entity['value']} (ì‹ ë¢°ë„: {entity['confidence']:.2f})")
        else:
            print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")