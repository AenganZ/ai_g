# pseudonymization/replacement.py
"""
ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ê°€ëª…í™” ì¹˜í™˜ ëª¨ë“ˆ
í† í° ê¸°ë°˜ ì¹˜í™˜ ì‹œìŠ¤í…œ: [PER_0], [ORG_0], [LOC_0] ë“±
"""

import re
import random
from collections import defaultdict
from typing import Dict, List, Any, Tuple

class WorkflowReplacementManager:
    """ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ í† í° ì¹˜í™˜ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.substitution_map = {}  # ì›ë³¸ â†’ í† í°
        self.reverse_map = {}       # í† í° â†’ ì›ë³¸  
        print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì¹˜í™˜ë§¤ë‹ˆì € ì´ˆê¸°í™”")
    
    def create_substitution_map(self, items: List[Dict[str, Any]], token_map: Dict[str, str]) -> Tuple[Dict[str, str], Dict[str, str]]:
        """ì¹˜í™˜ ë§µ ìƒì„± (í† í° ê¸°ë°˜)"""
        
        print(f"ğŸ—ºï¸ ì¹˜í™˜ ë§µ ìƒì„±: {len(items)}ê°œ í•­ëª©")
        
        substitution_map = {}
        reverse_map = {}
        
        for item in items:
            original = item['value']
            token = token_map.get(original)
            
            if token:
                substitution_map[original] = token
                reverse_map[token] = original
                print(f"ğŸ—ºï¸ ë§¤í•‘: '{original}' â†” {token}")
        
        self.substitution_map = substitution_map
        self.reverse_map = reverse_map
        
        print(f"ğŸ—ºï¸ ì¹˜í™˜ ë§µ ìƒì„± ì™„ë£Œ: {len(substitution_map)}ê°œ ë§¤í•‘")
        
        return substitution_map, reverse_map
    
    def restore_from_tokens(self, tokenized_text: str, reverse_map: Dict[str, str]) -> str:
        """í† í°ì„ ì›ë³¸ìœ¼ë¡œ ë³µì›"""
        
        print("ğŸ”„ í† í° ë³µì› ì‹œì‘")
        
        result = tokenized_text
        restored_count = 0
        
        # ëª¨ë“  í† í°ì„ ì›ë³¸ìœ¼ë¡œ ë³µì›
        for token, original in reverse_map.items():
            if token in result:
                count = result.count(token)
                result = result.replace(token, original)
                restored_count += count
                print(f"ğŸ”„ ë³µì›: {token} â†’ '{original}' ({count}ë²ˆ)")
        
        print(f"ğŸ”„ í† í° ë³µì› ì™„ë£Œ: {restored_count}ê°œ ë³µì›")
        
        return result

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_workflow_manager = None

def get_workflow_manager():
    """WorkflowReplacementManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global _workflow_manager
    if _workflow_manager is None:
        _workflow_manager = WorkflowReplacementManager()
    return _workflow_manager

def apply_tokenization(text: str, substitution_map: Dict[str, str]) -> str:
    """í† í°í™” ì ìš©"""
    if not substitution_map:
        return text
    
    print(f"ğŸ·ï¸ í† í°í™” ì ìš©: {len(substitution_map)}ê°œ ë§¤í•‘")
    
    result = text
    applied_count = 0
    
    # ê¸´ ë¬¸ìì—´ë¶€í„° ì¹˜í™˜ (ë¶€ë¶„ ë¬¸ìì—´ ë¬¸ì œ ë°©ì§€)
    sorted_items = sorted(substitution_map.items(), key=lambda x: len(x[0]), reverse=True)
    
    for original, token in sorted_items:
        if original in result:
            count = result.count(original)
            result = result.replace(original, token)
            applied_count += count
            print(f"ğŸ·ï¸ í† í°í™”: '{original}' â†’ {token} ({count}ë²ˆ)")
    
    print(f"ğŸ·ï¸ í† í°í™” ì™„ë£Œ: {applied_count}ê°œ ì ìš©")
    return result

def restore_from_tokens(tokenized_text: str, reverse_map: Dict[str, str]) -> str:
    """í† í°ì„ ì›ë³¸ìœ¼ë¡œ ë³µì›"""
    manager = get_workflow_manager()
    return manager.restore_from_tokens(tokenized_text, reverse_map)

def create_detailed_mapping_report(substitution_map: Dict[str, str], reverse_map: Dict[str, str]) -> str:
    """ìƒì„¸ ë§¤í•‘ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = "ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ í† í° ë§¤í•‘ ë¦¬í¬íŠ¸\n"
    report += "=" * 50 + "\n"
    
    by_type = defaultdict(list)
    
    for original, token in substitution_map.items():
        # í† í°ì—ì„œ íƒ€ì… ì¶”ì¶œ
        if '[PER_' in token:
            pii_type = 'ì´ë¦„'
        elif '[ORG_' in token:
            pii_type = 'íšŒì‚¬'
        elif '[LOC_' in token:
            pii_type = 'ì£¼ì†Œ'
        elif '[EMAIL_' in token:
            pii_type = 'ì´ë©”ì¼'
        elif '[PHONE_' in token:
            pii_type = 'ì „í™”ë²ˆí˜¸'
        elif '[AGE_' in token:
            pii_type = 'ë‚˜ì´'
        elif '[RRN_' in token:
            pii_type = 'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸'
        elif '[CARD_' in token:
            pii_type = 'ì‹ ìš©ì¹´ë“œ'
        elif '[ACCT_' in token:
            pii_type = 'ê³„ì¢Œë²ˆí˜¸'
        else:
            pii_type = 'ê¸°íƒ€'
        
        by_type[pii_type].append((original, token))
    
    for pii_type, mappings in by_type.items():
        report += f"\n{pii_type} ({len(mappings)}ê°œ í•­ëª©):\n"
        for original, token in mappings:
            report += f"   â€¢ {original} â†” {token}\n"
    
    report += f"\nì „ì²´: {len(substitution_map)}ê°œ PII í† í°í™”ë¨\n"
    report += "ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ì–‘ë°©í–¥ ë§¤í•‘ ì™„ë£Œ\n"
    
    return report

# í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í´ë˜ìŠ¤
class ReplacementManager(WorkflowReplacementManager):
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def assign_replacements(self, items: List[Dict[str, Any]]) -> Tuple[Dict[str, str], Dict[str, str]]:
        """í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ - í† í° ë§µ ì‚¬ìš©"""
        
        # í† í° ë§µ ìƒì„±
        token_map = {}
        type_counters = {}
        
        type_prefixes = {
            'ì´ë¦„': 'PER',
            'íšŒì‚¬': 'ORG', 
            'ì£¼ì†Œ': 'LOC',
            'ì´ë©”ì¼': 'EMAIL',
            'ì „í™”ë²ˆí˜¸': 'PHONE',
            'ë‚˜ì´': 'AGE',
            'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸': 'RRN',
            'ì‹ ìš©ì¹´ë“œ': 'CARD',
            'ê³„ì¢Œë²ˆí˜¸': 'ACCT'
        }
        
        for item in items:
            pii_type = item['type']
            pii_value = item['value']
            
            if pii_type not in type_counters:
                type_counters[pii_type] = 0
            
            prefix = type_prefixes.get(pii_type, 'MISC')
            token = f"[{prefix}_{type_counters[pii_type]}]"
            
            token_map[pii_value] = token
            type_counters[pii_type] += 1
        
        return self.create_substitution_map(items, token_map)

# í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
def apply_replacements_smart(text: str, substitution_map: Dict[str, str]) -> str:
    return apply_tokenization(text, substitution_map)

def apply_replacements(text: str, substitution_map: Dict[str, str]) -> str:
    return apply_tokenization(text, substitution_map)

def restore_text_smart(tokenized_text: str, reverse_map: Dict[str, str]) -> str:
    return restore_from_tokens(tokenized_text, reverse_map)

def restore_text(tokenized_text: str, reverse_map: Dict[str, str]) -> str:
    return restore_from_tokens(tokenized_text, reverse_map)

def remove_duplicates(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì¤‘ë³µ ì œê±° (ìœ„ì¹˜ ê¸°ë°˜)"""
    unique_items = []
    seen_positions = set()
    
    for item in items:
        position_key = (item['start'], item['end'], item['value'])
        if position_key not in seen_positions:
            unique_items.append(item)
            seen_positions.add(position_key)
    
    return unique_items

# ì›Œí¬í”Œë¡œìš° í•µì‹¬ í•¨ìˆ˜ export
__all__ = [
    'ReplacementManager',
    'WorkflowReplacementManager',
    'apply_replacements',
    'apply_replacements_smart',
    'apply_tokenization',
    'restore_text',
    'restore_text_smart', 
    'restore_from_tokens',
    'create_detailed_mapping_report',
    'remove_duplicates'
]