# pseudonymization/normalizers.py - ì´ë¦„/ì£¼ì†Œ íƒì§€ ê°•í™” ë²„ì „ (ì¡°ì‚¬ ì œì™¸ ìˆ˜ì •, ì „í™”ë²ˆí˜¸ ì¤‘ë³µ í•´ê²°)
import re
import asyncio
from typing import Optional, Dict, List, Any

# â­ ê°•í™”ëœ ì´ë©”ì¼ ì •ê·œì‹ íŒ¨í„´ë“¤
EMAIL_PATTERNS = [
    # ê¸°ë³¸ íŒ¨í„´ (ë‹¨ì–´ ê²½ê³„ ì—†ìŒ)
    re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}'),
    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë‚´ ì´ë©”ì¼ íŒ¨í„´
    re.compile(r'[A-Za-z0-9][A-Za-z0-9._%+-]*@[A-Za-z0-9][A-Za-z0-9.-]*\.[A-Za-z]{2,}'),
    # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ì´ë©”ì¼ ë³µì› íŒ¨í„´
    re.compile(r'[A-Za-z0-9._%+-]+\s*@\s*[A-Za-z0-9.-]+\s*\.\s*[A-Za-z]{2,}'),
]

AGE_RX = re.compile(r"\b(\d{1,3})\s*(?:ì„¸|ì‚´)?\b")
PHONE_NUM_ONLY = re.compile(r"\D+")
PHONE_PATTERN = re.compile(r'010[-\s]?\d{4}[-\s]?\d{4}')

# â­â­â­ ëŒ€í­ ê°•í™”ëœ ì´ë¦„ íƒì§€ íŒ¨í„´ (ì¡°ì‚¬ ì œì™¸ ìˆ˜ì •) â­â­â­
NAME_PATTERNS = [
    # ê¸°ì¡´ íŒ¨í„´ë“¤ (ì¡°ì‚¬ ì œì™¸ ê°•í™”)
    re.compile(r'ì´ë¦„ì€\s*([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )(ë‹˜|ì”¨)?'),
    re.compile(r'ì €ëŠ”\s*([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )(ë‹˜|ì”¨)?'),
    re.compile(r'([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ë¼ê³ )(ë‹˜|ì”¨)?\s*ì…ë‹ˆë‹¤'),
    re.compile(r'([ê°€-í£]{2,4})(ì´ì—ìš”|ì˜ˆìš”|ì´ì•¼|ì•¼)'),
    re.compile(r'([ê°€-í£]{2,4})(ë‹˜|ì”¨)(?![ê°€-í£])'),
    re.compile(r'ì•ˆë…•í•˜ì„¸ìš”,?\s*(?:ì €ëŠ”\s*)?([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )(ë‹˜|ì”¨)?'),
    re.compile(r'([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )(ë‹˜|ì”¨)?\s*ê³ ê°'),
    re.compile(r'([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )(ë‹˜|ì”¨)?\s*íšŒì›'),
    
    # â­ ìƒˆë¡œ ì¶”ê°€ëœ ê°•í™” íŒ¨í„´ë“¤ (ì¡°ì‚¬ ì œì™¸ ê°•í™”) â­
    re.compile(r'ë‚˜\s*([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )ì¸ë°'),           # "ë‚˜ ì˜¤ìˆ˜ë¯¼ì¸ë°"
    re.compile(r'([ê°€-í£]{2,4})ì´ê³ (?![ê°€-í£])'),     # "ê¹€ìˆ˜í•œì´ê³ " (ì¡°ì‚¬ "ì´ê³ ") - ì´ë¦„ë§Œ ì¶”ì¶œ
    re.compile(r'([ê°€-í£]{2,4})(?![ê°€-í£]|ì´ê³ )ë¼ê³ \s*(?:í•©ë‹ˆë‹¤|í•´ìš”|ë¶ˆëŸ¬)'),  # "ê¹€ì² ìˆ˜ë¼ê³  í•©ë‹ˆë‹¤"
    re.compile(r'([ê°€-í£]{2,4})(?=\s*(?:ì´|ê°€)\s*(?:ë§í–ˆë‹¤|í–ˆë‹¤|ì™”ë‹¤|ê°”ë‹¤|ìˆë‹¤))'),  # "ê¹€ì² ìˆ˜ê°€ ì™”ë‹¤"
    re.compile(r'([ê°€-í£]{2,4})(?=\s*(?:ì€|ëŠ”)\s*(?:í•™ìƒ|ì§ì¥ì¸|ì˜ì‚¬|ì„ ìƒ))'),  # "ê¹€ì² ìˆ˜ëŠ” í•™ìƒ"
    re.compile(r'([ê°€-í£]{2,4})(?=\s*(?:ì„|ë¥¼)\s*(?:ë§Œë‚¬ë‹¤|ë´¤ë‹¤|ì°¾ì•„))'),      # "ê¹€ì² ìˆ˜ë¥¼ ë§Œë‚¬ë‹¤"
    re.compile(r'([ê°€-í£]{2,4})(?=\s*(?:ì—ê²Œ|í•œí…Œ)\s*(?:ë§í–ˆë‹¤|ì¤¬ë‹¤|ì „í™”))'),   # "ê¹€ì² ìˆ˜ì—ê²Œ ì „í™”"
    re.compile(r'([ê°€-í£]{2,4})\s*(?:ì”¨|ë‹˜)\s*(?:ì´|ê°€)'),                   # "ê¹€ì² ìˆ˜ì”¨ê°€"
    re.compile(r'ì œ\s*ì´ë¦„ì€\s*([ê°€-í£]{2,3})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )'),  # â­ í•µì‹¬ ìˆ˜ì •: "ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜" - ì¡°ì‚¬ ì œì™¸
    re.compile(r'ë‚´\s*ì´ë¦„ì€\s*([ê°€-í£]{2,3})(?![ê°€-í£]|ì´ê³ |ì´ì—ìš”|ì…ë‹ˆë‹¤|ë¼ê³ )'),  # â­ í•µì‹¬ ìˆ˜ì •: "ë‚´ ì´ë¦„ì€ ê¹€ì² ìˆ˜" - ì¡°ì‚¬ ì œì™¸
    re.compile(r'([ê°€-í£]{2,4})\s*(?:ë¼ëŠ”|ì´ë¼ëŠ”)\s*(?:ì´ë¦„|ì‚¬ëŒ)'),            # "ê¹€ì² ìˆ˜ë¼ëŠ” ì´ë¦„"
    re.compile(r'([ê°€-í£]{2,4})\s*(?:ì´ë¼ê³ |ë¼ê³ )\s*(?:í•˜ëŠ”ë°|í•´ì„œ)'),           # "ê¹€ì² ìˆ˜ë¼ê³  í•´ì„œ"
]

# NER ëª¨ë¸ import (ì„ íƒì )
try:
    from .model import extract_entities_with_ner, is_ner_available, is_ner_loaded
    NER_AVAILABLE = True
except ImportError:
    NER_AVAILABLE = False

def get_pools():
    """pools.pyì—ì„œ ë°ì´í„°í’€ ê°€ì ¸ì˜¤ê¸°"""
    from .pools import get_pools
    return get_pools()

def smart_clean_korean_text(text: str, preserve_context: bool = True) -> str:
    """ìŠ¤ë§ˆíŠ¸ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´) - ì¡°ì‚¬ ì œê±° ê°•í™”"""
    if not text:
        return text
    
    cleaned = text.strip()
    
    # â­ ì¡°ì‚¬ ì œê±° ê°•í™” - preserve_contextì™€ ê´€ê³„ì—†ì´ ëª…í™•í•œ ì¡°ì‚¬ëŠ” ì œê±°
    if not preserve_context:
        # ì¡°ì‚¬ íŒ¨í„´ (ëì— ì˜¤ëŠ” ì¡°ì‚¬ë“¤)
        particles = ['ì´ê³ ', 'ì´ì—ìš”', 'ì…ë‹ˆë‹¤', 'ë¼ê³ ', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”', 'ì˜', 'ì™€', 'ê³¼', 'ì—', 'ì—ê²Œ', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ']
        # ì¡´ì¹­ì€ ë³´ì¡´ (ë‹˜, ì”¨ëŠ” ì œê±°í•˜ì§€ ì•ŠìŒ)
        
        # ëì— ìˆëŠ” ì¡°ì‚¬ë“¤ë§Œ ì œê±° (ì¡´ì¹­ì€ ë³´ì¡´)
        for particle in sorted(particles, key=len, reverse=True):
            if cleaned.endswith(particle) and len(cleaned) > len(particle) + 1:  # ìµœì†Œ 2ê¸€ìëŠ” ë‚¨ê²¨ì•¼ í•¨
                without_particle = cleaned[:-len(particle)]
                if len(without_particle) >= 2:
                    cleaned = without_particle
                    break
    
    return cleaned

def is_valid_korean_name(name: str, include_honorifics: bool = True) -> bool:
    """í•œêµ­ì–´ ì´ë¦„ ìœ íš¨ì„± ê²€ì¦ (ì¡´ì¹­ í¬í•¨ ì˜µì…˜) - ê°•í™”"""
    pools = get_pools()
    
    if not name or len(name) < 2 or len(name) > 5:  # ì¡´ì¹­ í¬í•¨í•˜ë©´ ìµœëŒ€ 5ê¸€ì
        return False
    
    # â­ ì¡°ì‚¬ ì œê±° í›„ ê²€ì¦ ê°•í™”
    base_name = name
    has_honorific = False
    
    # ì¡°ì‚¬ ì œê±°
    particles_to_remove = ['ì´ê³ ', 'ì´ì—ìš”', 'ì…ë‹ˆë‹¤', 'ë¼ê³ ']
    for particle in particles_to_remove:
        if base_name.endswith(particle):
            base_name = base_name[:-len(particle)]
            break
    
    if include_honorifics:
        if base_name.endswith('ë‹˜') or base_name.endswith('ì”¨'):
            base_name = base_name[:-1]
            has_honorific = True
    
    if len(base_name) < 2 or len(base_name) > 4:
        return False
    
    # í•œê¸€ë§Œ í—ˆìš©
    if not all('\uac00' <= char <= '\ud7af' for char in base_name):
        return False
    
    # ìˆ«ì í¬í•¨ ì œì™¸
    if any(char.isdigit() for char in base_name):
        return False
    
    # ì œì™¸ ë‹¨ì–´ë“¤
    if base_name in pools.name_exclude_words:
        return False
    
    # í™•ì¥ëœ ì¼ë°˜ëª…ì‚¬ ëª©ë¡
    common_nouns = {
        "ê³ ê°", "ì†ë‹˜", "íšŒì›", "ì„ ìƒ", "êµìˆ˜", "ì˜ì‚¬", "ì§ì›", "í•™ìƒ",
        "ì¹œêµ¬", "ì„ ë°°", "í›„ë°°", "ë™ë£Œ", "ê°€ì¡±", "ë¶€ëª¨", "ìë…€", "í˜•ì œ",
        "ìë§¤", "ì‚¬ëŒ", "ë¶„ë“¤", "ì—¬ëŸ¬ë¶„", "ëª¨ë“ ", "ëª¨ë‘", "ì „ë¶€", "ì¼ë¶€",
        "ë‹´ë‹¹ì", "ì±…ì„ì", "ê´€ë¦¬ì", "ìš´ì˜ì", "ê°œë°œì", "ì„¤ê³„ì", "ê¸°íšì",
        "ìƒë‹´ì›", "ì•ˆë‚´ì›", "ì ‘ìˆ˜ì›", "ëŒ€ë¦¬", "ê³¼ì¥", "ë¶€ì¥", "íŒ€ì¥", "ì‹¤ì¥",
        "ì°¨ì¥", "ì´ì‚¬", "ìƒë¬´", "ì „ë¬´", "ì‚¬ì¥", "ëŒ€í‘œ", "íšŒì¥", "ì˜ì¥",
        "ì´ë²ˆ", "ë‹¤ìŒ", "ì €ë²ˆ", "ì²˜ìŒ", "ë§ˆì§€ë§‰", "ì²«ì§¸", "ë‘˜ì§¸", "ì…‹ì§¸",
        "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì§€ê¸ˆ", "ë‚˜ì¤‘", "ì•ì„œ", "ì´í›„", "ì´ì „",
        "ê·¸ë¶„", "ì´ë¶„", "ì €ë¶„", "ëˆ„êµ°ê°€", "ì•„ë¬´ë‚˜", "ëª¨ë“ ", "ê°ì", "ì„œë¡œ",
        "í˜¼ì", "í•¨ê»˜", "ê°™ì´", "ë”°ë¡œ", "ë³„ë„", "ê°œë³„", "ê³µë™", "ì „ì²´",
        # â­ ì¶”ê°€ ì œì™¸ ë‹¨ì–´ë“¤
        "ë­ë¼", "ì„¸ì•„", "íƒœí‰", "ë™ì´"
    }
    
    if base_name in common_nouns:
        return False
    
    # ì§€ì—­ëª… ì œì™¸ (ê°•í™”)
    pools = get_pools()
    all_regions = set(pools.provinces + pools.cities + pools.roads)
    if base_name in all_regions:
        return False
    
    # â­ ë™ëª…(æ´å) íŒ¨í„´ ì œì™¸ (íƒœí‰ë™, ì‹ ì •ë™ ë“±)
    if base_name.endswith('ë™') and len(base_name) >= 3:
        return False
    
    # í•œêµ­ì–´ ì„±ì”¨ í™•ì¸ (ì„ íƒì  ê°•í™”)
    common_surnames = {
        "ê¹€", "ì´", "ë°•", "ìµœ", "ì •", "ê°•", "ì¡°", "ìœ¤", "ì¥", "ì„", "í•œ", "ì˜¤", 
        "ì„œ", "ì‹ ", "ê¶Œ", "í™©", "ì•ˆ", "ì†¡", "ì „", "í™", "ê³ ", "ë¬¸", "ì–‘", "ì†"
    }
    
    # 2ê¸€ì ì´ë¦„ì¸ë° ì„±ì”¨ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ì›€
    if len(base_name) == 2 and base_name[0] not in common_surnames:
        if base_name not in pools.real_names:
            return False
    
    return True

# ===== PII íƒì§€ í•¨ìˆ˜ë“¤ (ê°•í™”ë¨) =====

def detect_emails(text: str) -> List[Dict[str, Any]]:
    """â­ ê°•í™”ëœ ì´ë©”ì¼ íƒì§€"""
    items = []
    seen_emails = set()
    
    print(f"ğŸ“§ ê°•í™”ëœ ì´ë©”ì¼ íƒì§€ ì‹œì‘: '{text}'")
    
    # 1ë‹¨ê³„: ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ì´ë©”ì¼ íƒì§€
    for i, pattern in enumerate(EMAIL_PATTERNS):
        print(f"  íŒ¨í„´ {i+1} ì‹œë„: {pattern.pattern}")
        for match in pattern.finditer(text):
            raw_email = match.group()
            
            # ê³µë°± ì œê±°í•˜ì—¬ ì •ê·œí™”
            clean_email = re.sub(r'\s+', '', raw_email)
            
            print(f"    ë°œê²¬: '{raw_email}' â†’ ì •ë¦¬: '{clean_email}'")
            
            # ê¸°ë³¸ ì´ë©”ì¼ ìœ íš¨ì„± ê²€ì‚¬
            if '@' in clean_email and '.' in clean_email.split('@')[1]:
                email_parts = clean_email.split('@')
                if len(email_parts) == 2 and len(email_parts[0]) > 0 and len(email_parts[1]) > 2:
                    if clean_email not in seen_emails:
                        seen_emails.add(clean_email)
                        
                        items.append({
                            "type": "ì´ë©”ì¼",
                            "value": clean_email.lower(),  # ì†Œë¬¸ìë¡œ ì •ê·œí™”
                            "start": match.start(),
                            "end": match.end(),
                            "confidence": 0.95,
                            "source": f"normalizers-ì´ë©”ì¼-íŒ¨í„´{i+1}",
                            "original_match": raw_email
                        })
                        print(f"    âœ… ì´ë©”ì¼ ì¶”ê°€: '{clean_email.lower()}'")
                    else:
                        print(f"    ğŸ”„ ì¤‘ë³µ ì´ë©”ì¼: '{clean_email}'")
            else:
                print(f"    âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë©”ì¼: '{clean_email}'")
    
    # 2ë‹¨ê³„: íŠ¹ìˆ˜ í•œêµ­ì–´ íŒ¨í„´ (ì´ë©”ì¼ í‚¤ì›Œë“œ í¬í•¨)
    email_context_patterns = [
        r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})(?:ìœ¼?ë¡œ|ì—ê²Œ|ë¥¼|ì„)?\s*(?:ë©”ì¼|ì´ë©”ì¼|ë©”ì‹œì§€|ì—°ë½)',
        r'(?:ë©”ì¼|ì´ë©”ì¼|ì—°ë½)\s*(?:ì€|ëŠ”|ì„|ë¥¼)?\s*([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})',
        r'([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,})(?:ìœ¼?ë¡œ|ì—|ë¥¼)\s*(?:ë³´ë‚´|ì „ì†¡|ë°œì†¡)'
    ]
    
    for i, pattern in enumerate(email_context_patterns):
        print(f"  ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ {i+1} ì‹œë„...")
        for match in re.finditer(pattern, text, re.IGNORECASE):
            email = match.group(1).lower()
            print(f"    ì»¨í…ìŠ¤íŠ¸ ë°œê²¬: '{email}'")
            
            if email not in seen_emails:
                seen_emails.add(email)
                items.append({
                    "type": "ì´ë©”ì¼",
                    "value": email,
                    "start": match.start(1),
                    "end": match.end(1),
                    "confidence": 0.90,
                    "source": f"normalizers-ì´ë©”ì¼-ì»¨í…ìŠ¤íŠ¸{i+1}",
                    "original_match": match.group()
                })
                print(f"    âœ… ì»¨í…ìŠ¤íŠ¸ ì´ë©”ì¼ ì¶”ê°€: '{email}'")
    
    print(f"ğŸ“§ ê°•í™”ëœ ì´ë©”ì¼ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    return items

def detect_phones(text: str) -> List[Dict[str, Any]]:
    """ì „í™”ë²ˆí˜¸ íƒì§€ (ì •í™•ë„ ê°œì„ , ì—°ì† ìˆ«ì í˜•íƒœ í¬í•¨)"""
    items = []
    seen_phones = set()
    
    # 1. ê¸°ì¡´ íŒ¨í„´ (í•˜ì´í”ˆ/ê³µë°± í¬í•¨)
    for match in PHONE_PATTERN.finditer(text):
        phone = match.group()
        normalized_phone = phone.replace(' ', '').replace('-', '')
        
        if len(normalized_phone) == 11 and normalized_phone.startswith('010'):
            formatted_phone = f"{normalized_phone[:3]}-{normalized_phone[3:7]}-{normalized_phone[7:]}"
            
            if formatted_phone not in seen_phones:
                seen_phones.add(formatted_phone)
                items.append({
                    "type": "ì „í™”ë²ˆí˜¸",
                    "value": formatted_phone,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95,
                    "source": "normalizers-ì „í™”ë²ˆí˜¸",
                    "normalized": normalized_phone  # â­ ì •ê·œí™”ëœ ê°’ ì¶”ê°€
                })
                print(f"  âœ… ì „í™”ë²ˆí˜¸ (íŒ¨í„´): '{phone}' â†’ '{formatted_phone}' (ì •ê·œí™”: {normalized_phone})")
    
    # 2. â­ ì—°ì†ëœ 11ìë¦¬ ìˆ«ì íŒ¨í„´ (01012345678)
    continuous_pattern = re.compile(r'\b(010\d{8})\b')
    for match in continuous_pattern.finditer(text):
        phone = match.group()
        
        # ì´ë¯¸ ìœ„ì˜ íŒ¨í„´ìœ¼ë¡œ íƒì§€ëœ ê²ƒê³¼ ì¤‘ë³µì¸ì§€ í™•ì¸
        if phone not in [item.get("normalized", "") for item in items]:
            formatted_phone = f"{phone[:3]}-{phone[3:7]}-{phone[7:]}"
            
            if formatted_phone not in seen_phones:
                seen_phones.add(formatted_phone)
                items.append({
                    "type": "ì „í™”ë²ˆí˜¸",
                    "value": formatted_phone,  # â­ í•­ìƒ í¬ë§·íŒ…ëœ í˜•íƒœë¡œ ì €ì¥
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95,
                    "source": "normalizers-ì „í™”ë²ˆí˜¸-ì—°ì†",
                    "normalized": phone,  # â­ ì •ê·œí™”ëœ ê°’ ì¶”ê°€
                    "original_form": "continuous"  # ì›ë³¸ì´ ì—°ì† í˜•íƒœì˜€ìŒì„ í‘œì‹œ
                })
                print(f"  âœ… ì „í™”ë²ˆí˜¸ (ì—°ì†): '{phone}' â†’ '{formatted_phone}' (ì •ê·œí™”: {phone})")
    
    return items

def detect_ages(text: str) -> List[Dict[str, Any]]:
    """ë‚˜ì´ íƒì§€ (ì—„ê²©í•œ ê²€ì¦)"""
    items = []
    seen_ages = set()
    
    for match in AGE_RX.finditer(text):
        age_str = match.group(1)
        
        if age_str in seen_ages:
            continue
        
        try:
            age = int(age_str)
            if 1 <= age <= 120 and len(age_str) <= 2:
                start_pos = max(0, match.start() - 10)
                end_pos = min(len(text), match.end() + 10)
                context = text[start_pos:end_pos]
                
                age_keywords = ['ì„¸', 'ì‚´', 'ë‚˜ì´', 'ì—°ë ¹', 'ë§Œ', 'ë…„ìƒ', 'ì˜¬í•´']
                if any(keyword in context for keyword in age_keywords):
                    seen_ages.add(age_str)
                    items.append({
                        "type": "ë‚˜ì´",
                        "value": age_str,
                        "start": match.start(),
                        "end": match.end(),
                        "confidence": 1.0,
                        "source": "normalizers-ë‚˜ì´"
                    })
        except ValueError:
            continue
    return items

def detect_names(text: str) -> List[Dict[str, Any]]:
    """â­â­â­ ëŒ€í­ ê°•í™”ëœ ì´ë¦„ íƒì§€ (ì¡°ì‚¬ ì œì™¸ ê°•í™”) â­â­â­"""
    items = []
    detected_names = set()
    
    print(f"ğŸ” ëŒ€í­ ê°•í™”ëœ ì´ë¦„ íƒì§€ ì‹œì‘ (ì¡°ì‚¬ ì œì™¸): '{text}'")
    
    # 1. íŒ¨í„´ ê¸°ë°˜ íƒì§€ (ì¡°ì‚¬ ì œì™¸ ê°•í™”)
    for i, pattern in enumerate(NAME_PATTERNS):
        for match in pattern.finditer(text):
            # ê·¸ë£¹ 1: ì´ë¦„, ê·¸ë£¹ 2: ì¡´ì¹­ (ì˜µì…˜)
            base_name = match.group(1)
            
            # â­ ê·¸ë£¹ 2ê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            try:
                honorific = match.group(2) if match.lastindex > 1 and match.group(2) else ""
            except IndexError:
                honorific = ""
            
            # â­ ì¡°ì‚¬ ì œê±° í›„ì²˜ë¦¬ ê°•í™”
            cleaned_base_name = smart_clean_korean_text(base_name, preserve_context=False)
            full_name = cleaned_base_name + (honorific or "")
            
            print(f"  íŒ¨í„´ {i+1}: ì›ë³¸ '{base_name}' â†’ ì •ë¦¬ '{cleaned_base_name}' + ì¡´ì¹­ '{honorific}' = '{full_name}'")
            
            # â­ ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ìœ íš¨ì„± ê²€ì‚¬ (ê°•í™”ë¨)
            if not is_valid_korean_name(cleaned_base_name, include_honorifics=False):
                print(f"    âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ê¸°ë³¸ ì´ë¦„: '{cleaned_base_name}'")
                continue
            
            # â­ ì¡´ì¹­ì´ ìˆëŠ” ê²½ìš° ì „ì²´ ì´ë¦„ë„ ê²€ì‚¬
            if honorific and not is_valid_korean_name(full_name, include_honorifics=True):
                print(f"    âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì „ì²´ ì´ë¦„: '{full_name}'")
                continue
            
            # ì¤‘ë³µ ì œê±° (ê¸°ë³¸ ì´ë¦„ ê¸°ì¤€)
            if cleaned_base_name in detected_names:
                print(f"    ğŸ”„ ì¤‘ë³µ ì œê±°: '{cleaned_base_name}'")
                continue
            
            # â­ ì¡´ì¹­ì´ ìˆëŠ” ê²½ìš° ì „ì²´ ì´ë¦„ì„ ì €ì¥, ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ë§Œ
            final_name = full_name if honorific else cleaned_base_name
            
            items.append({
                "type": "ì´ë¦„",
                "value": final_name,
                "start": match.start(1),
                "end": match.start(1) + len(cleaned_base_name) + len(honorific),  # â­ ì •í™•í•œ end ìœ„ì¹˜
                "confidence": 0.85,
                "source": f"normalizers-ì´ë¦„íŒ¨í„´-{i+1}",
                "has_honorific": bool(honorific),
                "base_name": cleaned_base_name,
                "honorific": honorific,
                "original_match": base_name  # ì›ë³¸ ë§¤ì¹˜ ê¸°ë¡
            })
            detected_names.add(cleaned_base_name)  # ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            print(f"    âœ… ì´ë¦„ íƒì§€: '{final_name}' (íŒ¨í„´ {i+1}: ê¸°ë³¸ '{cleaned_base_name}', ì¡´ì¹­ '{honorific}')")
    
    # 2. ì‹¤ëª… ëª©ë¡ ê¸°ë°˜ íƒì§€ (ì¡´ì¹­ í¬í•¨)
    pools = get_pools()
    for real_name in pools.real_names:
        if real_name in detected_names:
            continue
        
        # ê¸°ë³¸ ì´ë¦„ ë§¤ì¹­
        for match in re.finditer(re.escape(real_name), text):
            # ì•ë’¤ ë¬¸ë§¥ í™•ì¸í•˜ì—¬ ì¡´ì¹­ í¬í•¨ ì—¬ë¶€ íŒë‹¨
            start_pos = match.start()
            end_pos = match.end()
            
            # ë’¤ì— ì¡´ì¹­ì´ ìˆëŠ”ì§€ í™•ì¸
            if end_pos < len(text) and text[end_pos:end_pos+1] in ['ë‹˜', 'ì”¨']:
                full_name = real_name + text[end_pos]
                end_pos += 1
                has_honorific = True
            else:
                full_name = real_name
                has_honorific = False
            
            # â­ ë’¤ì— ì¡°ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì œì™¸
            if end_pos < len(text):
                next_chars = text[end_pos:end_pos+2]
                if any(next_chars.startswith(particle) for particle in ['ì´ê³ ', 'ì´ì—']):
                    print(f"  âš ï¸ ì‹¤ëª… ëª©ë¡: '{real_name}' ë’¤ì— ì¡°ì‚¬ ë°œê²¬, ì´ë¦„ë§Œ ì¶”ì¶œ")
            
            items.append({
                "type": "ì´ë¦„",
                "value": full_name,
                "start": start_pos,
                "end": end_pos,
                "confidence": 0.90,
                "source": "normalizers-ì‹¤ëª…ëª©ë¡",
                "has_honorific": has_honorific,
                "base_name": real_name,
                "honorific": text[end_pos-1] if has_honorific else ""
            })
            detected_names.add(real_name)
            print(f"  âœ… ì‹¤ëª… ëª©ë¡: '{full_name}' (ê¸°ë³¸: '{real_name}')")
    
    print(f"ğŸ” ëŒ€í­ ê°•í™”ëœ ì´ë¦„ íƒì§€ ì™„ë£Œ (ì¡°ì‚¬ ì œì™¸): {len(items)}ê°œ")
    return items

def detect_addresses(text: str) -> List[Dict[str, Any]]:
    """â­ ì£¼ì†Œ íƒì§€ ê°•í™” (ì¤‘ë³µ ì œê±° ê°œì„ )"""
    items = []
    pools = get_pools()
    all_addresses = []
    
    print(f"ğŸ  ê°•í™”ëœ ì£¼ì†Œ íƒì§€ ì‹œì‘: '{text}'")
    
    # 1. ë³µí•© ì£¼ì†Œ íŒ¨í„´ (ì¡°ì‚¬ í¬í•¨ ë²„ì „)
    for province in pools.provinces:
        complex_patterns = [
            rf'{re.escape(province)}(?:ì‹œ|ë„)?\s+[ê°€-í£]+(?:êµ¬|êµ°|ì‹œ)(?:ì—ì„œ|ì—|ë¡œ|ìœ¼ë¡œ)?',
            rf'{re.escape(province)}\s+[ê°€-í£]+(?:êµ¬|êµ°)(?:ì—ì„œ|ì—|ë¡œ|ìœ¼ë¡œ)?',
        ]
        
        for pattern in complex_patterns:
            for match in re.finditer(pattern, text):
                full_match = match.group()
                
                # â­ ì¡°ì‚¬ëŠ” ë¶„ë¦¬í•˜ë˜ ì»¨í…ìŠ¤íŠ¸ëŠ” ë³´ì¡´
                clean_match = re.sub(r'(ì—ì„œ|ì—|ë¡œ|ìœ¼ë¡œ)$', '', full_match).strip()
                
                print(f"  ë³µí•© íŒ¨í„´: '{full_match}' â†’ ì •ë¦¬: '{clean_match}'")
                
                all_addresses.append({
                    "province": province,
                    "value": clean_match,
                    "original_match": full_match,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95,
                    "priority": 1,  # â­ ë³µí•© ì£¼ì†Œê°€ ìµœìš°ì„ 
                    "has_particle": full_match != clean_match,
                    "is_complex": True  # â­ ë³µí•© ì£¼ì†Œ í”Œë˜ê·¸
                })
    
    # 2. ë‹¨ì¼ ì£¼ì†Œ íŒ¨í„´ (ë³µí•© ì£¼ì†Œê°€ ì—†ì„ ë•Œë§Œ)
    if not all_addresses:  # â­ ë³µí•© ì£¼ì†Œê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë‹¨ì¼ ì£¼ì†ŒëŠ” ìŠ¤í‚µ
        for province in pools.provinces:
            pattern = rf'{re.escape(province)}(?:ì‹œ|ë„)?(?:ì—ì„œ|ì—|ë¡œ|ìœ¼ë¡œ)?'
            for match in re.finditer(pattern, text):
                full_match = match.group()
                clean_match = re.sub(r'(ì—ì„œ|ì—|ë¡œ|ìœ¼ë¡œ)$', '', full_match).strip()
                
                print(f"  ë‹¨ì¼ íŒ¨í„´: '{full_match}' â†’ ì •ë¦¬: '{clean_match}'")
                
                start_pos = max(0, match.start() - 15)
                end_pos = min(len(text), match.end() + 15)
                context = text[start_pos:end_pos]
                
                address_keywords = ['ê±°ì£¼', 'ì‚´ê³ ', 'ìˆìŠµë‹ˆë‹¤', 'ìœ„ì¹˜', 'ì£¼ì†Œ', 'ì˜ˆì•½', 'ì§€ì—­']
                if any(keyword in context for keyword in address_keywords):
                    all_addresses.append({
                        "province": province,
                        "value": clean_match,
                        "original_match": full_match,
                        "start": match.start(),
                        "end": match.end(),
                        "confidence": 0.80,
                        "priority": 2,
                        "has_particle": full_match != clean_match,
                        "is_complex": False
                    })
    
    # 3. â­ ì£¼ì†Œ ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì²˜ë¦¬
    all_addresses.sort(key=lambda x: (x["priority"], x["start"]))
    
    # ë³µí•© ì£¼ì†Œê°€ ìˆìœ¼ë©´ ê·¸ êµ¬ì„±ìš”ì†Œì¸ ë‹¨ì¼ ì£¼ì†Œë“¤ ì œì™¸
    complex_addresses = [addr for addr in all_addresses if addr.get("is_complex", False)]
    if complex_addresses:
        print(f"  ğŸ” ë³µí•© ì£¼ì†Œ ë°œê²¬: {len(complex_addresses)}ê°œ - êµ¬ì„±ìš”ì†Œ ì œì™¸ ì²˜ë¦¬")
        
        # ë³µí•© ì£¼ì†Œë§Œ ì‚¬ìš©
        for addr in complex_addresses:
            items.append({
                "type": "ì£¼ì†Œ",
                "value": addr["value"],  # ì •ë¦¬ëœ ì£¼ì†Œ
                "start": addr["start"],
                "end": addr["end"],
                "confidence": addr["confidence"],
                "source": "normalizers-ì£¼ì†Œ-ë³µí•©",
                "original_match": addr["original_match"],
                "has_particle": addr["has_particle"]
            })
            print(f"  âœ… ë³µí•© ì£¼ì†Œ: '{addr['value']}' (ì›ë³¸: '{addr['original_match']}')")
    else:
        print(f"  ğŸ” ë³µí•© ì£¼ì†Œ ì—†ìŒ - ê°œë³„ ì£¼ì†Œ ì‚¬ìš©")
        # ë³µí•© ì£¼ì†Œê°€ ì—†ì„ ë•Œë§Œ ê°œë³„ ì£¼ì†Œ ì‚¬ìš©
        for addr in all_addresses:
            items.append({
                "type": "ì£¼ì†Œ",
                "value": addr["value"],  # ì •ë¦¬ëœ ì£¼ì†Œ
                "start": addr["start"],
                "end": addr["end"],
                "confidence": addr["confidence"],
                "source": "normalizers-ì£¼ì†Œ-ê°œë³„",
                "original_match": addr["original_match"],
                "has_particle": addr["has_particle"]
            })
            print(f"  âœ… ê°œë³„ ì£¼ì†Œ: '{addr['value']}' (ì›ë³¸: '{addr['original_match']}')")
    
    print(f"ğŸ  ê°•í™”ëœ ì£¼ì†Œ íƒì§€ ì™„ë£Œ: {len(items)}ê°œ")
    return items

def detect_with_ner_supplement(text: str, existing_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """NER ëª¨ë¸ ë³´ì™„ íƒì§€ (ì¤‘ë³µ ì œê±° ê°•í™”, ì „í™”ë²ˆí˜¸ ì •ê·œí™”)"""
    if not NER_AVAILABLE:
        return []
    
    try:
        existing_values = set()
        existing_complex_addresses = set()
        existing_normalized_phones = set()  # â­ ì „í™”ë²ˆí˜¸ ì •ê·œí™” ê°’ ì €ì¥
        
        for item in existing_items:
            # ê¸°ë³¸ ì´ë¦„ê³¼ ì¡´ì¹­ í¬í•¨ ì´ë¦„ ëª¨ë‘ ê¸°ë¡
            if item["type"] == "ì´ë¦„":
                base_value = item.get("base_name", item["value"])
                existing_values.add(base_value)
                existing_values.add(item["value"])
            # â­ ì „í™”ë²ˆí˜¸ëŠ” ì •ê·œí™”í•´ì„œ ì¤‘ë³µ ì²´í¬ (ê°œì„ )
            elif item["type"] == "ì „í™”ë²ˆí˜¸":
                existing_values.add(item["value"])
                # itemì—ì„œ normalized ê°’ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ì§ì ‘ ì •ê·œí™”
                normalized_phone = item.get("normalized") or re.sub(r'[^0-9]', '', item["value"])
                existing_normalized_phones.add(normalized_phone)
                print(f"  â­ ê¸°ì¡´ ì „í™”ë²ˆí˜¸ ì •ê·œí™”: '{item['value']}' â†’ '{normalized_phone}'")
            else:
                existing_values.add(item["value"])
                
            # â­ ë³µí•© ì£¼ì†Œ êµ¬ì„±ìš”ì†Œ ê¸°ë¡
            if item["type"] == "ì£¼ì†Œ" and item.get("source", "").endswith("-ë³µí•©"):
                # "ëŒ€ì „ ì¤‘êµ¬" â†’ ["ëŒ€ì „", "ì¤‘êµ¬"] ì¶”ê°€
                address_parts = item["value"].split()
                existing_complex_addresses.update(address_parts)
        
        # â­ ë³µí•© ì£¼ì†Œê°€ ìˆìœ¼ë©´ ê·¸ êµ¬ì„±ìš”ì†Œë“¤ë„ ì œì™¸
        all_existing_values = existing_values.union(existing_complex_addresses)
        
        print(f"ğŸ” NER ë³´ì™„: ê¸°ì¡´ ê°’ë“¤ ì œì™¸ - {len(all_existing_values)}ê°œ")
        for val in sorted(all_existing_values):
            print(f"  ì œì™¸: '{val}'")
        
        print(f"ğŸ” NER ë³´ì™„: ì •ê·œí™”ëœ ì „í™”ë²ˆí˜¸ ì œì™¸ - {len(existing_normalized_phones)}ê°œ")
        for phone in sorted(existing_normalized_phones):
            print(f"  ì •ê·œí™” ì œì™¸: '{phone}'")
        
        ner_entities = extract_entities_with_ner(text)
        
        supplementary_items = []
        for entity in ner_entities:
            entity_type = entity.get('type', '')
            raw_value = entity.get('value', '')
            confidence = entity.get('confidence', 0.0)
            
            # â­ ìŠ¤ë§ˆíŠ¸ ì •ë¦¬ (ì¡°ì‚¬ ì œê±°)
            clean_value = smart_clean_korean_text(raw_value, preserve_context=False)
            
            # â­ ì „í™”ë²ˆí˜¸ ì¤‘ë³µ ì²´í¬ ë° ì •ê·œí™” ê°•í™”
            if entity_type == "ì „í™”ë²ˆí˜¸":
                # ìˆ«ìë§Œ ì¶”ì¶œí•´ì„œ ì •ê·œí™”
                normalized_ner_phone = re.sub(r'[^0-9]', '', clean_value)
                if normalized_ner_phone in existing_normalized_phones:
                    print(f"    NER ì œì™¸: '{clean_value}' (ì •ê·œí™”ëœ ì „í™”ë²ˆí˜¸ ì¤‘ë³µ: '{normalized_ner_phone}')")
                    continue
                else:
                    # â­ NER ì „í™”ë²ˆí˜¸ë„ í¬ë§·íŒ…ëœ í˜•íƒœë¡œ ì €ì¥
                    if len(normalized_ner_phone) == 11 and normalized_ner_phone.startswith('010'):
                        formatted_ner_phone = f"{normalized_ner_phone[:3]}-{normalized_ner_phone[3:7]}-{normalized_ner_phone[7:]}"
                        clean_value = formatted_ner_phone
                        print(f"    â­ NER ì „í™”ë²ˆí˜¸ í¬ë§·íŒ…: '{raw_value}' â†’ '{formatted_ner_phone}' (ì •ê·œí™”: '{normalized_ner_phone}')")
                    else:
                        print(f"    â­ NER ì „í™”ë²ˆí˜¸ ì •ê·œí™”: '{clean_value}' â†’ '{normalized_ner_phone}'")
            
            # â­ ê°•í™”ëœ ì¤‘ë³µ ì²´í¬ (ê¸°ì¡´ ë¡œì§)
            if clean_value in all_existing_values or not clean_value:
                print(f"    NER ì œì™¸: '{clean_value}' (ê¸°ì¡´ í•­ëª©ê³¼ ì¤‘ë³µ)")
                continue
            
            if confidence > 0.9:
                if entity_type == "ì´ë¦„":
                    if not is_valid_korean_name(clean_value, include_honorifics=True):
                        print(f"    NER ì œì™¸: '{clean_value}' (ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¦„)")
                        continue
                    if not all('\uac00' <= char <= '\ud7af' or char in 'ì”¨ë‹˜' for char in clean_value):
                        print(f"    NER ì œì™¸: '{clean_value}' (í•œê¸€ì´ ì•„ë‹˜)")
                        continue
                
                # ì¡´ì¹­ ë¶„ë¦¬
                base_name = clean_value
                honorific = ""
                if clean_value.endswith('ë‹˜') or clean_value.endswith('ì”¨'):
                    base_name = clean_value[:-1]
                    honorific = clean_value[-1]
                
                # â­ ì „í™”ë²ˆí˜¸ì¸ ê²½ìš° normalized ê°’ë„ ì¶”ê°€
                item_data = {
                    "type": entity_type,
                    "value": clean_value,
                    "start": entity.get('start', 0),
                    "end": entity.get('start', 0) + len(clean_value),
                    "confidence": confidence,
                    "source": f"NER-ë³´ì™„",
                    "has_honorific": bool(honorific),
                    "base_name": base_name,
                    "honorific": honorific
                }
                
                # â­ ì „í™”ë²ˆí˜¸ì¸ ê²½ìš° normalized ê°’ ì¶”ê°€
                if entity_type == "ì „í™”ë²ˆí˜¸":
                    item_data["normalized"] = re.sub(r'[^0-9]', '', clean_value)
                
                supplementary_items.append(item_data)
                print(f"    âœ… NER ë³´ì™„: '{clean_value}' ({entity_type})")
        
        print(f"ğŸ” NER ë³´ì™„ ì™„ë£Œ: {len(supplementary_items)}ê°œ ì¶”ê°€")
        return supplementary_items
        
    except Exception as e:
        print(f"NER ë³´ì™„ íƒì§€ ì˜¤ë¥˜: {e}")
        return []

async def detect_pii_all(text: str) -> List[Dict[str, Any]]:
    """í†µí•© PII íƒì§€ í•¨ìˆ˜ (ì´ë¦„/ì£¼ì†Œ ê°•í™”, ì¡°ì‚¬ ì œì™¸)"""
    print(f"\nğŸ” === ê°•í™”ëœ PII íƒì§€ ì‹œì‘ (ì´ë¦„/ì£¼ì†Œ ê°•í™”, ì¡°ì‚¬ ì œì™¸) ===")
    print(f"ğŸ“ ì…ë ¥: '{text}'")
    
    all_items = []
    
    # 1ë‹¨ê³„: normalizers ê¸°ë°˜ ì£¼ìš” íƒì§€ 
    all_items.extend(detect_emails(text))
    all_items.extend(detect_phones(text))
    all_items.extend(detect_names(text))      # â­ ì¡°ì‚¬ ì œì™¸ ê°•í™”ë¨
    all_items.extend(detect_addresses(text))  # â­ ì¤‘ë³µ ì œê±° ê°•í™”ë¨
    all_items.extend(detect_ages(text))
    
    # 2ë‹¨ê³„: NER ë³´ì™„ (ì¤‘ë³µ ì œê±° ê°•í™”)
    if NER_AVAILABLE:
        ner_supplement = detect_with_ner_supplement(text, all_items)
        all_items.extend(ner_supplement)
    
    # 3ë‹¨ê³„: â­ ìµœì¢… ì¤‘ë³µ ì œê±° (ê°œì„ ë¨)
    seen_items = set()
    final_items = []
    
    for item in all_items:
        # ì´ë¦„ì˜ ê²½ìš° ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
        if item["type"] == "ì´ë¦„":
            base_name = item.get("base_name", item["value"])
            key = (item["type"], base_name)
        else:
            key = (item["type"], item["value"])
        
        if key not in seen_items:
            final_items.append(item)
            seen_items.add(key)
            print(f"âœ… ìµœì¢… í•­ëª©: {item['type']} '{item['value']}' (ì¶œì²˜: {item.get('source', 'unknown')})")
        else:
            print(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {item['type']} '{item['value']}'")
    
    print(f"ğŸ” === ê°•í™”ëœ PII íƒì§€ ì™„ë£Œ (ì´ë¦„/ì£¼ì†Œ ê°•í™”, ì¡°ì‚¬ ì œì™¸): {len(final_items)}ê°œ ===\n")
    return final_items

# ===== ê¸°ì¡´ ì •ê·œí™” í•¨ìˆ˜ë“¤ (ìœ ì§€) =====

def normalize_entities(raw_entities: List[Dict[str, Optional[str]]]) -> List[Dict[str, Optional[str]]]:
    """ì—”í„°í‹° ì •ê·œí™”"""
    out = []
    for e in raw_entities:
        ent = {
            "name": norm_name(e.get("name")) if isinstance(e.get("name"), str) else None,
            "age": norm_age(str(e.get("age"))) if e.get("age") is not None else None,
            "phone": norm_phone(e.get("phone")) if isinstance(e.get("phone"), str) else None,
            "email": norm_email(e.get("email")) if isinstance(e.get("email"), str) else None,
            "address": norm_address(e.get("address")) if isinstance(e.get("address"), str) else None
        }
        ent = cross_check(ent)
        out.append(ent)
    return out

def norm_age(val: Optional[str]) -> Optional[str]:
    """ë‚˜ì´ ê°’ì„ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ì •ê·œí™”"""
    if not val:
        return None
    m = AGE_RX.search(val)
    return m.group(1) if m else None

def to_digits(s: str) -> str:
    """ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ"""
    return PHONE_NUM_ONLY.sub("", s)

def norm_phone(val: Optional[str]) -> Optional[str]:
    """ì „í™”ë²ˆí˜¸ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    if not val:
        return None
    raw = val.strip()
    if raw.startswith("+82"):
        digits = to_digits(raw)
        if digits.startswith("8210"):
            digits = "0" + digits[2:]
        elif digits.startswith("82"):
            digits = "0" + digits[2:]
    else:
        digits = to_digits(raw)

    if digits.startswith("010") and len(digits) == 11:
        return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
    if digits[:3] in {"011","016","017","018","019"}:
        if len(digits) == 10:
            return f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
        if len(digits) == 11:
            return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"

    tidy = re.sub(r"\s+", " ", raw).replace("â€“","-").replace("â€”","-")
    return tidy

def norm_email(val: Optional[str]) -> Optional[str]:
    """ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì†Œë¬¸ìë¡œ ì •ê·œí™”"""
    if not val:
        return None
    val = val.strip()
    if "@" not in val:
        return None
    return val.lower()

def norm_name(val: Optional[str]) -> Optional[str]:
    """ì´ë¦„ì—ì„œ ê³µë°± ì •ë¦¬ (ì¡´ì¹­ ë³´ì¡´)"""
    if not val:
        return None
    return smart_clean_korean_text(re.sub(r"\s+", " ", val).strip(), preserve_context=True)

def norm_address(val: Optional[str]) -> Optional[str]:
    """ì£¼ì†Œë¥¼ ì •ë¦¬í•˜ì—¬ ì •ê·œí™” (ì¡°ì‚¬ë§Œ ì œê±°)"""
    if not val:
        return None
    cleaned = re.sub(r"\s+", " ", val).strip()
    # ëì˜ ì¡°ì‚¬ë§Œ ì œê±°
    cleaned = re.sub(r'(ì—ì„œ|ì—|ë¡œ|ìœ¼ë¡œ)$', '', cleaned).strip()
    return cleaned

def cross_check(entity: Dict[str, Optional[str]]) -> Dict[str, Optional[str]]:
    """ì—”í‹°í‹° ê°„ êµì°¨ ê²€ì¦ (â­ ì´ë©”ì¼ ê°•í™”)"""
    addr = entity.get("address")
    email = entity.get("email")
    
    # ì£¼ì†Œ í•„ë“œì—ì„œ ì´ë©”ì¼ ì¶”ì¶œ ê°•í™”
    if addr and ("@" in addr):
        # ê°•í™”ëœ ì´ë©”ì¼ íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ
        for pattern in EMAIL_PATTERNS:
            match = pattern.search(addr)
            if match and not email:
                found_email = match.group().strip().lower()
                entity["email"] = found_email
                entity["address"] = None
                print(f"ğŸ“§ êµì°¨ê²€ì¦: ì£¼ì†Œì—ì„œ ì´ë©”ì¼ ì¶”ì¶œ '{found_email}'")
                break
        else:
            # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì£¼ì†Œ í•„ë“œ ì œê±°
            entity["address"] = None
    
    return entity

# í˜¸í™˜ì„± í•¨ìˆ˜ë“¤
def detect_pii_enhanced(text: str):
    return asyncio.run(detect_pii_all(text))

def detect_with_ner(text: str):
    return asyncio.run(detect_pii_all(text))

def detect_with_regex(text: str):
    return asyncio.run(detect_pii_all(text))

def detect_names_from_csv(text: str):
    return detect_names(text)

def detect_addresses_from_csv(text: str):
    return detect_addresses(text)

def merge_detections(*detection_lists):
    merged = []
    for detection_list in detection_lists:
        if detection_list:
            merged.extend(detection_list)
    
    seen = set()
    unique = []
    for item in merged:
        # ì´ë¦„ì˜ ê²½ìš° ê¸°ë³¸ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
        if item.get("type") == "ì´ë¦„":
            base_name = item.get("base_name", item["value"])
            key = (item["type"], base_name)
        else:
            key = (item["type"], item["value"])
        
        if key not in seen:
            unique.append(item)
            seen.add(key)
    
    return unique