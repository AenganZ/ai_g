# app_standalone.py - ë‹¨ì¼ íŒŒì¼ ì™„ì „ ë²„ì „ (Windows í˜¸í™˜)
import os
import re
import json
import time
import random
import asyncio
import threading
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, request, jsonify
from flask_cors import CORS

# NER ëª¨ë¸ ê´€ë ¨ import (ì„ íƒì )
try:
    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
    import torch
    import pandas as pd
    NER_AVAILABLE = True
    print("âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    NER_AVAILABLE = False
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íƒì§€ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# ===== ì„¤ì • =====
LOG_FILE = "pseudo-log.json"
MAX_LOGS = 100

# ===== Flask ì„¤ì • =====
app = Flask(__name__)
CORS(app)

# ===== ì „ì—­ ë³€ìˆ˜ =====
name_pool = []
full_name_pool = []
fake_name_pool = []
email_pool = []
phone_pool = []
address_pool = []
company_pool = []

ner_pipeline = None
model_loaded = False
manager_initialized = False

# ===== ë¡œê¹… ìœ í‹¸ë¦¬í‹° =====
def append_json_to_file(path: str, new_entry: Dict[str, Any]) -> None:
    """JSON ì—”íŠ¸ë¦¬ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì¶”ê°€"""
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = {"logs": []}
    except:
        data = {"logs": []}
    
    if "logs" not in data or not isinstance(data["logs"], list):
        data["logs"] = []
    
    data["logs"].append(new_entry)
    
    # ë¡œê·¸ ê°œìˆ˜ ì œí•œ
    if len(data["logs"]) > MAX_LOGS:
        data["logs"] = data["logs"][-MAX_LOGS:]
    
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ===== ì •ê·œì‹ íŒ¨í„´ =====
EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
PHONE_PATTERN = re.compile(r'010[-\s]?\d{4}[-\s]?\d{4}')
AGE_PATTERN = re.compile(r'\b(\d{1,2})\s*(?:ì„¸|ì‚´)\b')

# ê°•í™”ëœ ì´ë¦„ íŒ¨í„´
NAME_PATTERNS = [
    re.compile(r'ì´ë¦„ì€\s*([ê°€-í£]{2,4})'),
    re.compile(r'([ê°€-í£]{2,4})\s*ì…ë‹ˆë‹¤'),
    re.compile(r'ì €ëŠ”\s*([ê°€-í£]{2,4})'),
    re.compile(r'([ê°€-í£]{2,4})(?:ì´ì—ìš”|ì˜ˆìš”|ì´ì•¼|ì•¼)'),
    re.compile(r'([ê°€-í£]{2,4})(?:ì…ë‹ˆë‹¤|ì´ë‹¤)'),
    re.compile(r'ì•ˆë…•í•˜ì„¸ìš”,?\s*(?:ì €ëŠ”\s*)?([ê°€-í£]{2,4})'),
    re.compile(r'([ê°€-í£]{2,4})ì´ê³ '),
    re.compile(r'([ê°€-í£]{2,4})ì´ë©°'),
    re.compile(r'([ê°€-í£]{2,4})ë¼ê³ \s*í•©ë‹ˆë‹¤'),
    re.compile(r'([ê°€-í£]{2,4})ë¼ê³ \s*í•´ìš”'),
    re.compile(r'([ê°€-í£]{2,4})(?:ë‹˜|ì”¨)'),
]

# ì£¼ì†Œ íŒ¨í„´
ADDRESS_PATTERNS = [
    re.compile(r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°)\s+[ê°€-í£\s\d,-]+(?:ë™|ë¡œ|ê°€|ë²ˆì§€|ì¸µ|í˜¸)'),
    re.compile(r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°)'),
]

# ===== ë°ì´í„°í’€ ë¡œë“œ =====
def load_data_pools():
    """ëª¨ë“  ë°ì´í„°í’€ ì´ˆê¸°í™”"""
    global name_pool, full_name_pool, fake_name_pool
    global email_pool, phone_pool, address_pool, company_pool
    
    print("ğŸ“‚ ë°ì´í„°í’€ ë¡œë”© ì¤‘...")
    
    # ì´ë¦„í’€ ë¡œë“œ (name.csvê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    try:
        if os.path.exists('name.csv') and NER_AVAILABLE:
            import pandas as pd
            df = pd.read_csv('name.csv', encoding='utf-8')
            name_pool = df['ì´ë¦„'].tolist()[:1000]  # ìµœëŒ€ 1000ê°œ
            print(f"âœ… name.csvì—ì„œ {len(name_pool)}ê°œ ì´ë¦„ ë¡œë“œ")
        else:
            name_pool = [
                'ë¯¼ì¤€', 'ì„œì¤€', 'ë„ìœ¤', 'ì˜ˆì¤€', 'ì‹œìš°', 'ì£¼ì›', 'í•˜ì¤€', 'ì§€í˜¸',
                'ì§€í›„', 'ì¤€ìš°', 'í˜„ìš°', 'ì¤€ì„œ', 'ë„í˜„', 'ì§€í›ˆ', 'ê±´ìš°', 'ìš°ì§„',
                'ì„œìœ¤', 'ì§€ìš°', 'ì„œí˜„', 'í•˜ì€', 'ì˜ˆì€', 'ìœ¤ì„œ', 'ì§€ë¯¼', 'ì±„ì›'
            ]
            print(f"âœ… ê¸°ë³¸ ì´ë¦„í’€ ì‚¬ìš©: {len(name_pool)}ê°œ")
    except Exception as e:
        print(f"âŒ ì´ë¦„í’€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        name_pool = ['ë¯¼ì¤€', 'ì„œì¤€', 'ì§€ìš°', 'ì„œí˜„']
    
    # í•œêµ­ ì„±ì”¨
    surnames = [
        'ê¹€', 'ì´', 'ë°•', 'ìµœ', 'ì •', 'ê°•', 'ì¡°', 'ìœ¤', 'ì¥', 'ì„',
        'í•œ', 'ì˜¤', 'ì„œ', 'ì‹ ', 'ê¶Œ', 'í™©', 'ì•ˆ', 'ì†¡', 'ë¥˜', 'ì „'
    ]
    
    # ì„±+ì´ë¦„ ì¡°í•© ìƒì„±
    full_name_pool = []
    for surname in surnames:
        for name in name_pool[:50]:  # ë©”ëª¨ë¦¬ ì ˆì•½
            full_name_pool.append(surname + name)
    
    # ê°€ëª… ì´ë¦„ í’€ ìƒì„±
    fake_words = ['ê°€ëª…', 'ìµëª…', 'ë¬´ëª…', 'ì°¨ëª…', 'ë³„ëª…', 'í…ŒìŠ¤íŠ¸', 'ìƒ˜í”Œ', 'ë”ë¯¸']
    fake_name_pool = [surname + fake_word for surname in surnames for fake_word in fake_words]
    
    # ì´ë©”ì¼í’€
    email_domains = ['gmail.com', 'naver.com', 'daum.net', 'kakao.com']
    email_prefixes = ['user', 'test', 'hello', 'work', 'info', 'office']
    email_pool = []
    for i in range(100):
        prefix = random.choice(email_prefixes) + str(i + 1000)
        domain = random.choice(email_domains)
        email_pool.append(f"{prefix}@{domain}")
    
    # ì „í™”ë²ˆí˜¸í’€
    phone_pool = [f"010-{i//100:04d}-{i%100:04d}" for i in range(1000, 2000)]
    
    # ì£¼ì†Œí’€
    address_pool = [
        'ì„œìš¸ì‹œ ê°•ë‚¨êµ¬', 'ì„œìš¸ì‹œ ì„œì´ˆêµ¬', 'ì„œìš¸ì‹œ ì†¡íŒŒêµ¬', 'ì„œìš¸ì‹œ ê°•ë™êµ¬',
        'ì„œìš¸ì‹œ ë§ˆí¬êµ¬', 'ì„œìš¸ì‹œ ìš©ì‚°êµ¬', 'ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬', 'ë¶€ì‚°ì‹œ ë¶€ì‚°ì§„êµ¬',
        'ëŒ€êµ¬ì‹œ ì¤‘êµ¬', 'ëŒ€êµ¬ì‹œ ë™êµ¬', 'ì¸ì²œì‹œ ë‚¨ë™êµ¬', 'ì¸ì²œì‹œ ë¶€í‰êµ¬',
        'ê²½ê¸°ë„ ìˆ˜ì›ì‹œ', 'ê²½ê¸°ë„ ì„±ë‚¨ì‹œ', 'ëŒ€ì „ì‹œ ì„œêµ¬', 'ê´‘ì£¼ì‹œ ì„œêµ¬'
    ]
    
    # íšŒì‚¬í’€
    company_pool = [
        'ì‚¼ì„±ì „ì', 'LGì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ìë™ì°¨', 'KIA', 'í¬ìŠ¤ì½”',
        'ë„·ë§ˆë¸”', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„', 'ì¿ íŒ¡', 'ë°°ë‹¬ì˜ë¯¼ì¡±', 'í† ìŠ¤'
    ]
    
    print(f"âœ… ë°ì´í„°í’€ ë¡œë“œ ì™„ë£Œ")
    print(f"   ğŸ“› ì´ë¦„: {len(name_pool)}ê°œ")
    print(f"   ğŸ‘¤ ì„±+ì´ë¦„: {len(full_name_pool)}ê°œ")
    print(f"   ğŸ­ ê°€ëª…ì´ë¦„: {len(fake_name_pool)}ê°œ")

def load_ner_model():
    """NER ëª¨ë¸ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ)"""
    global ner_pipeline, model_loaded
    
    if not NER_AVAILABLE:
        print("âŒ NER ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        return False
    
    try:
        print("ğŸ”„ NER ëª¨ë¸ ë¡œë”© ì¤‘... (monologg/koelectra-base-v3-naver-ner)")
        
        model_name = "monologg/koelectra-base-v3-naver-ner"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForTokenClassification.from_pretrained(model_name)
        
        ner_pipeline = pipeline(
            "ner", 
            model=model, 
            tokenizer=tokenizer,
            aggregation_strategy="simple",
            device=0 if torch.cuda.is_available() else -1
        )
        
        model_loaded = True
        print("âœ… NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ NER ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        model_loaded = False
        return False

def map_ner_label_to_pii_type(label: str) -> Optional[str]:
    """NER ë¼ë²¨ì„ PII íƒ€ì…ìœ¼ë¡œ ë§¤í•‘"""
    mapping = {
        'PER': 'ì´ë¦„',
        'PERSON': 'ì´ë¦„',
        'LOC': 'ì£¼ì†Œ',
        'LOCATION': 'ì£¼ì†Œ',
        'ORG': 'íšŒì‚¬',
        'ORGANIZATION': 'íšŒì‚¬'
    }
    return mapping.get(label)

def detect_pii_enhanced(text: str) -> List[Dict[str, Any]]:
    """ê°•í™”ëœ PII íƒì§€ (ë™ê¸° ë²„ì „)"""
    items = []
    
    print(f"ğŸ” PII ë¶„ì„: {text[:50]}...")
    
    # 1. NER ëª¨ë¸ ì‚¬ìš© (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if model_loaded and ner_pipeline:
        try:
            ner_results = ner_pipeline(text)
            
            for entity in ner_results:
                entity_type = entity['entity_group']
                entity_text = entity['word']
                confidence = entity['score']
                start = entity['start']
                end = entity['end']
                
                if confidence > 0.7:
                    pii_type = map_ner_label_to_pii_type(entity_type)
                    if pii_type:
                        items.append({
                            "type": pii_type,
                            "value": entity_text,
                            "start": start,
                            "end": end,
                            "confidence": confidence,
                            "source": "NER"
                        })
        except Exception as e:
            print(f"âŒ NER ëª¨ë¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    # 2. ì •ê·œì‹ ê¸°ë°˜ íƒì§€
    # ì´ë©”ì¼
    for match in EMAIL_PATTERN.finditer(text):
        items.append({
            "type": "ì´ë©”ì¼",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "Regex"
        })
    
    # ì „í™”ë²ˆí˜¸
    for match in PHONE_PATTERN.finditer(text):
        items.append({
            "type": "ì „í™”ë²ˆí˜¸",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "Regex"
        })
    
    # ë‚˜ì´
    for match in AGE_PATTERN.finditer(text):
        items.append({
            "type": "ë‚˜ì´",
            "value": match.group(1),
            "start": match.start(),
            "end": match.end(),
            "confidence": 0.9,
            "source": "Regex"
        })
    
    # ì´ë¦„ íŒ¨í„´
    for pattern in NAME_PATTERNS:
        for match in pattern.finditer(text):
            name = match.group(1)
            if len(name) >= 2 and len(name) <= 4:
                items.append({
                    "type": "ì´ë¦„",
                    "value": name,
                    "start": match.start(1),
                    "end": match.end(1),
                    "confidence": 0.75,
                    "source": "Pattern"
                })
    
    # ì£¼ì†Œ íŒ¨í„´
    for pattern in ADDRESS_PATTERNS:
        for match in pattern.finditer(text):
            items.append({
                "type": "ì£¼ì†Œ",
                "value": match.group(),
                "start": match.start(),
                "end": match.end(),
                "confidence": 0.9,
                "source": "Regex"
            })
    
    # 3. ë°ì´í„°í’€ ê¸°ë°˜ íƒì§€
    if full_name_pool:
        for full_name in full_name_pool[:500]:  # ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ
            if full_name in text:
                start_idx = text.find(full_name)
                items.append({
                    "type": "ì´ë¦„",
                    "value": full_name,
                    "start": start_idx,
                    "end": start_idx + len(full_name),
                    "confidence": 0.8,
                    "source": "FullNamePool"
                })
    
    # ì¤‘ë³µ ì œê±°
    unique_items = []
    seen = set()
    for item in sorted(items, key=lambda x: x['start']):
        key = (item['type'], item['value'], item['start'])
        if key not in seen:
            seen.add(key)
            unique_items.append(item)
    
    return unique_items

def assign_realistic_values(items: List[Dict[str, Any]]) -> Dict[str, str]:
    """ì‹¤ì œ ë°ì´í„°í’€ì—ì„œ ëŒ€ì²´ê°’ í• ë‹¹"""
    substitution_map = {}
    
    for item in items:
        pii_type = item['type']
        original_value = item['value']
        
        if original_value in substitution_map:
            continue
        
        if pii_type == "ì´ë¦„":
            replacement = random.choice(fake_name_pool) if fake_name_pool else "ê¹€ê°€ëª…"
        elif pii_type == "ì´ë©”ì¼":
            replacement = random.choice(email_pool) if email_pool else "test@example.com"
        elif pii_type == "ì „í™”ë²ˆí˜¸":
            replacement = random.choice(phone_pool) if phone_pool else "010-0000-0000"
        elif pii_type == "ì£¼ì†Œ":
            replacement = random.choice(address_pool) if address_pool else "ì„œìš¸ì‹œ ê°•ë‚¨êµ¬"
        elif pii_type == "íšŒì‚¬":
            replacement = random.choice(company_pool) if company_pool else "í…ŒìŠ¤íŠ¸íšŒì‚¬"
        elif pii_type == "ë‚˜ì´":
            replacement = str(random.randint(20, 65))
        else:
            replacement = f"[{pii_type.upper()}_MASKED]"
        
        substitution_map[original_value] = replacement
        item['replacement'] = replacement
    
    return substitution_map

def create_masked_text(original_text: str, items: List[Dict[str, Any]]) -> str:
    """ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
    replacements = [(item['value'], item.get('replacement', 'MASKED')) 
                   for item in items if item['value']]
    
    # ê¸´ ê²ƒë¶€í„° ì¹˜í™˜ (ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)
    replacements.sort(key=lambda x: len(x[0]), reverse=True)
    
    masked_text = original_text
    for original, replacement in replacements:
        masked_text = masked_text.replace(original, replacement)
    
    return masked_text

def pseudonymize_text(original_prompt: str) -> Dict[str, Any]:
    """ë©”ì¸ ê°€ëª…í™” í•¨ìˆ˜"""
    try:
        items = detect_pii_enhanced(original_prompt)
        substitution_map = assign_realistic_values(items)
        reverse_map = {v: k for k, v in substitution_map.items()}
        masked_prompt = create_masked_text(original_prompt, items)
        
        detection = {
            "contains_pii": len(items) > 0,
            "items": items,
            "model_used": "NER + Regex + NamePool + FullNamePool" if model_loaded else "Regex + NamePool + FullNamePool"
        }
        
        return {
            "masked_prompt": masked_prompt,
            "detection": detection,
            "substitution_map": substitution_map,
            "reverse_map": reverse_map
        }
    
    except Exception as e:
        print(f"âŒ ê°€ëª…í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return {
            "masked_prompt": original_prompt,
            "detection": {"contains_pii": False, "items": []},
            "substitution_map": {},
            "reverse_map": {}
        }

def initialize_manager():
    """ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
    global manager_initialized
    
    if manager_initialized:
        return
    
    print("ğŸš€ PseudonymizationManager ì´ˆê¸°í™” ì¤‘...")
    
    # ë°ì´í„°í’€ ë¡œë“œ
    load_data_pools()
    
    # NER ëª¨ë¸ ë°±ê·¸ë¼ìš´ë“œ ë¡œë“œ
    if NER_AVAILABLE:
        threading.Thread(target=load_ner_model, daemon=True).start()
    
    manager_initialized = True
    print("âœ… PseudonymizationManager ì´ˆê¸°í™” ì™„ë£Œ!")

# ===== Flask ë¼ìš°íŠ¸ =====
@app.route("/", methods=["GET", "OPTIONS"])
def root():
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response
    
    # ë§¤ë‹ˆì € ì´ˆê¸°í™”
    if not manager_initialized:
        initialize_manager()
    
    return jsonify({
        "message": "GenAI Pseudonymizer (AenganZ Enhanced)", 
        "version": "2.0.0",
        "framework": "Flask (Standalone)",
        "detection_method": "NER + Regex + DataPools",
        "ner_model_loaded": model_loaded,
        "data_pools": {
            "names": len(name_pool),
            "full_names": len(full_name_pool),
            "fake_names": len(fake_name_pool),
            "emails": len(email_pool)
        },
        "timestamp": datetime.now().isoformat()
    })

@app.route("/health", methods=["GET", "OPTIONS"])
def health():
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response
    
    return jsonify({
        "status": "ok",
        "method": "enhanced_detection",
        "ready": manager_initialized,
        "timestamp": datetime.now().isoformat()
    })

@app.route("/pseudonymize", methods=["POST", "OPTIONS"])
def pseudonymize():
    # CORS preflight ìš”ì²­ ì²˜ë¦¬
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response

    # ë§¤ë‹ˆì € ì´ˆê¸°í™”
    if not manager_initialized:
        initialize_manager()

    # JSON íŒŒì‹±
    try:
        data = request.get_json(force=True, silent=False)
    except Exception as e:
        response = jsonify(ok=False, error=f"invalid_json: {e}")
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 400

    if not isinstance(data, dict):
        response = jsonify(ok=False, error="payload_must_be_object")
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 400

    original_prompt = data.get("prompt", "")
    req_id = data.get("id", "")

    if not original_prompt.strip():
        response = jsonify(ok=False, error="empty_prompt")
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 400

    print(f"\n" + "="*60)
    print(f"ğŸ” ê°€ëª…í™” ìš”ì²­: {datetime.now().strftime('%H:%M:%S')}")
    print(f"ğŸ†” ID: {req_id}")
    print(f"ğŸ“„ ì›ë¬¸: {original_prompt}")

    try:
        # ê°€ëª…í™” ì²˜ë¦¬
        result = pseudonymize_text(original_prompt)
        
        masked_prompt = result["masked_prompt"]
        detection = result["detection"]
        substitution_map = result.get("substitution_map", {})
        reverse_map = result.get("reverse_map", {})

        # ë¡œê·¸ ì €ì¥
        log_entry = {
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "remote_addr": request.remote_addr,
            "path": request.path,
            "input": {
                "id": req_id,
                "prompt": original_prompt
            },
            "detection": detection,
            "substitution_map": substitution_map,
            "reverse_map": reverse_map,
            "performance": {
                "items_detected": len(detection.get("items", []))
            }
        }
        append_json_to_file(LOG_FILE, log_entry)

        print(f"âœ… ê°€ëª…í™” ì™„ë£Œ ({len(detection.get('items', []))}ê°œ íƒì§€)")
        print(f"ğŸ”„ ëŒ€ì²´ ë§µ: {substitution_map}")
        print("="*60)

        # ì‘ë‹µ ìƒì„±
        response_data = {
            "ok": True,
            "original_prompt": original_prompt,
            "masked_prompt": masked_prompt,
            "detection": detection,
            "substitution_map": substitution_map,
            "reverse_map": reverse_map,
            "mapping": detection.get("items", [])
        }
        
        response = jsonify(response_data)
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response

    except Exception as e:
        print(f"âŒ ê°€ëª…í™” ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        error_response = {
            "ok": False,
            "error": str(e),
            "original_prompt": original_prompt,
            "masked_prompt": original_prompt,
            "detection": {"contains_pii": False, "items": []},
            "substitution_map": {},
            "reverse_map": {},
            "mapping": []
        }
        
        response = jsonify(error_response)
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500

@app.route("/prompt_logs", methods=["GET", "OPTIONS"])
def prompt_logs():
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            raw = f.read()
        json.loads(raw)  # ìœ íš¨ì„± ê²€ì‚¬
        
        response = app.response_class(
            response=raw,
            status=200,
            mimetype="application/json; charset=utf-8"
        )
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
        
    except FileNotFoundError:
        empty = {"logs": []}
        response = app.response_class(
            response=json.dumps(empty, ensure_ascii=False),
            status=200,
            mimetype="application/json; charset=utf-8"
        )
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
    except Exception as e:
        response = jsonify({"error": f"log_read_error: {e}"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500

@app.route("/prompt_logs", methods=["DELETE"])
def clear_logs():
    try:
        with open(LOG_FILE, "w", encoding="utf-8") as f:
            json.dump({"logs": []}, f, ensure_ascii=False)
        
        response = jsonify({"success": True, "message": "Logs cleared"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
    except Exception as e:
        response = jsonify({"success": False, "error": str(e)})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500

if __name__ == "__main__":
    print("ğŸ­ GenAI Pseudonymizer (AenganZ Enhanced - Standalone)")
    print("ğŸ”§ í”„ë ˆì„ì›Œí¬: Flask (ë‹¨ì¼ íŒŒì¼ ë²„ì „)")
    print("ğŸ§  íƒì§€ ë°©ì‹: NER + ì •ê·œì‹ + ë°ì´í„°í’€")
    print("ğŸ“› ê°€ëª…í™”: ì‹¤ì œ ë°ì´í„° ëŒ€ì²´")
    print("ğŸ”„ ë³µì›: ì–‘ë°©í–¥ ë§¤í•‘")
    print("ğŸŒ ì„œë²„ ì‹œì‘ ì¤‘...")
    
    try:
        app.run(
            host="127.0.0.1",
            port=5000,
            debug=True,
            threaded=True
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì„œë²„ ì¢…ë£Œ")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()