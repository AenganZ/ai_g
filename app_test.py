# app_test.py - ë‹¨ì¼ íŒŒì¼ ì™„ì „ ë²„ì „ (í•œê¸€ ë¡œê·¸ ì ìš©)
import os
import re
import json
import time
import random
import asyncio
import threading
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, request, jsonify
from flask_cors import CORS

# NER ëª¨ë¸ ê´€ë ¨ import (ì„ íƒì )
try:
    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
    import torch
    import pandas as pd
    NER_AVAILABLE = True
    print("âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    NER_AVAILABLE = False
    print("âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íƒì§€ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# ===== ì„¤ì • =====
LOG_FILE = "pseudo-log.json"
MAX_LOGS = 100

# ===== Flask ì„¤ì • =====
app = Flask(__name__)
CORS(app)

# ===== ì „ì—­ ë³€ìˆ˜ =====
name_pool = []
full_name_pool = []
fake_name_pool = []
email_pool = []
phone_pool = []
address_pool = []
company_pool = []

ner_pipeline = None
model_loaded = False
manager_initialized = False

# ===== ë¡œê¹… ìœ í‹¸ë¦¬í‹° =====
def append_json_to_file(path: str, new_entry: Dict[str, Any]) -> None:
    """JSON ì—”íŠ¸ë¦¬ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì¶”ê°€"""
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = {"logs": []}
    except:
        data = {"logs": []}
    
    if "logs" not in data or not isinstance(data["logs"], list):
        data["logs"] = []
    
    data["logs"].append(new_entry)
    
    # ë¡œê·¸ ê°œìˆ˜ ì œí•œ
    if len(data["logs"]) > MAX_LOGS:
        data["logs"] = data["logs"][-MAX_LOGS:]
    
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ===== ì •ê·œì‹ íŒ¨í„´ =====
EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
PHONE_PATTERN = re.compile(r'010[-\s]?\d{4}[-\s]?\d{4}')
AGE_PATTERN = re.compile(r'\b(\d{1,2})\s*(?:ì„¸|ì‚´)\b')

# ê°•í™”ëœ ì´ë¦„ íŒ¨í„´
NAME_PATTERNS = [
    re.compile(r'ì´ë¦„ì€\s*([ê°€-í£]{2,4})'),
    re.compile(r'([ê°€-í£]{2,4})\s*ì…ë‹ˆë‹¤'),
    re.compile(r'ì €ëŠ”\s*([ê°€-í£]{2,4})'),
    re.compile(r'([ê°€-í£]{2,4})(?:ì´ì—ìš”|ì˜ˆìš”|ì´ì•¼|ì•¼)'),
    re.compile(r'([ê°€-í£]{2,4})(?:ì…ë‹ˆë‹¤|ì´ë‹¤)'),
    re.compile(r'ì•ˆë…•í•˜ì„¸ìš”,?\s*(?:ì €ëŠ”\s*)?([ê°€-í£]{2,4})'),
    re.compile(r'([ê°€-í£]{2,4})ì´ê³ '),
    re.compile(r'([ê°€-í£]{2,4})ì´ë©°'),
    re.compile(r'([ê°€-í£]{2,4})ë¼ê³ \s*í•©ë‹ˆë‹¤'),
    re.compile(r'([ê°€-í£]{2,4})ë¼ê³ \s*í•´ìš”'),
    re.compile(r'([ê°€-í£]{2,4})(?:ë‹˜|ì”¨)'),
]

# ì£¼ì†Œ íŒ¨í„´
ADDRESS_PATTERNS = [
    re.compile(r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°)\s+[ê°€-í£\s\d,-]+(?:ë™|ë¡œ|ê°€|ë²ˆì§€|ì¸µ|í˜¸)'),
    re.compile(r'[ê°€-í£]+(?:ì‹œ|ë„|êµ¬|êµ°)'),
]

# ===== ë°ì´í„°í’€ ë¡œë“œ =====
def load_data_pools():
    """ëª¨ë“  ë°ì´í„°í’€ ì´ˆê¸°í™”"""
    global name_pool, full_name_pool, fake_name_pool
    global email_pool, phone_pool, address_pool, company_pool
    
    print("ğŸ“‚ ë°ì´í„°í’€ ë¡œë”© ì¤‘...")
    
    # ì´ë¦„í’€ ë¡œë“œ (name.csvê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    try:
        if os.path.exists('name.csv') and NER_AVAILABLE:
            import pandas as pd
            df = pd.read_csv('name.csv', encoding='utf-8')
            name_pool = df['ì´ë¦„'].tolist()[:1000]  # ìµœëŒ€ 1000ê°œ
            print(f"âœ… name.csvì—ì„œ {len(name_pool)}ê°œ ì´ë¦„ ë¡œë“œ")
        else:
            name_pool = [
                'ë¯¼ì¤€', 'ì„œì¤€', 'ë„ìœ¤', 'ì˜ˆì¤€', 'ì‹œìš°', 'ì£¼ì›', 'í•˜ì¤€', 'ì§€í˜¸',
                'ì§€í›„', 'ì¤€ìš°', 'í˜„ìš°', 'ì¤€ì„œ', 'ë„í˜„', 'ì§€í›ˆ', 'ê±´ìš°', 'ìš°ì§„',
                'ì„œìœ¤', 'ì§€ìš°', 'ì„œí˜„', 'í•˜ì€', 'ì˜ˆì€', 'ìœ¤ì„œ', 'ì§€ë¯¼', 'ì±„ì›'
            ]
            print(f"âœ… ê¸°ë³¸ ì´ë¦„í’€ ì‚¬ìš©: {len(name_pool)}ê°œ")
    except Exception as e:
        print(f"âŒ ì´ë¦„í’€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        name_pool = ['ë¯¼ì¤€', 'ì„œì¤€', 'ì§€ìš°', 'ì„œí˜„']
    
    # í•œêµ­ ì„±ì”¨
    surnames = [
        'ê¹€', 'ì´', 'ë°•', 'ìµœ', 'ì •', 'ê°•', 'ì¡°', 'ìœ¤', 'ì¥', 'ì„',
        'í•œ', 'ì˜¤', 'ì„œ', 'ì‹ ', 'ê¶Œ', 'í™©', 'ì•ˆ', 'ì†¡', 'ë¥˜', 'ì „'
    ]
    
    # ì„±+ì´ë¦„ ì¡°í•© ìƒì„±
    full_name_pool = []
    for surname in surnames:
        for name in name_pool[:50]:  # ë©”ëª¨ë¦¬ ì ˆì•½
            full_name_pool.append(surname + name)
    
    # ê°€ëª… ì´ë¦„ í’€ ìƒì„±
    fake_words = ['ê°€ëª…', 'ìµëª…', 'ë¬´ëª…', 'ì°¨ëª…', 'ë³„ëª…', 'í…ŒìŠ¤íŠ¸', 'ìƒ˜í”Œ', 'ë”ë¯¸']
    fake_name_pool = [surname + fake_word for surname in surnames for fake_word in fake_words]
    
    # ì´ë©”ì¼í’€
    email_domains = ['gmail.com', 'naver.com', 'daum.net', 'kakao.com']
    email_prefixes = ['user', 'test', 'hello', 'work', 'info', 'office']
    email_pool = []
    for i in range(100):
        prefix = random.choice(email_prefixes) + str(i + 1000)
        domain = random.choice(email_domains)
        email_pool.append(f"{prefix}@{domain}")
    
    # ì „í™”ë²ˆí˜¸í’€
    phone_pool = [f"010-{i//100:04d}-{i%100:04d}" for i in range(1000, 2000)]
    
    # ì£¼ì†Œí’€
    address_pool = [
        'ì„œìš¸ì‹œ ê°•ë‚¨êµ¬', 'ì„œìš¸ì‹œ ì„œì´ˆêµ¬', 'ì„œìš¸ì‹œ ì†¡íŒŒêµ¬', 'ì„œìš¸ì‹œ ê°•ë™êµ¬',
        'ì„œìš¸ì‹œ ë§ˆí¬êµ¬', 'ì„œìš¸ì‹œ ìš©ì‚°êµ¬', 'ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬', 'ë¶€ì‚°ì‹œ ë¶€ì‚°ì§„êµ¬',
        'ëŒ€êµ¬ì‹œ ì¤‘êµ¬', 'ëŒ€êµ¬ì‹œ ë™êµ¬', 'ì¸ì²œì‹œ ë‚¨ë™êµ¬', 'ì¸ì²œì‹œ ë¶€í‰êµ¬',
        'ê²½ê¸°ë„ ìˆ˜ì›ì‹œ', 'ê²½ê¸°ë„ ì„±ë‚¨ì‹œ', 'ëŒ€ì „ì‹œ ì„œêµ¬', 'ê´‘ì£¼ì‹œ ì„œêµ¬'
    ]
    
    # íšŒì‚¬í’€
    company_pool = [
        'ì‚¼ì„±ì „ì', 'LGì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'í˜„ëŒ€ìë™ì°¨', 'ê¸°ì•„',
        'í¬ìŠ¤ì½”', 'NAVER', 'ì¹´ì¹´ì˜¤', 'ì‚¼ì„±SDI', 'LGí™”í•™'
    ]
    
    print("âœ… ë°ì´í„°í’€ ë¡œë”© ì™„ë£Œ")

def load_ner_model():
    """NER ëª¨ë¸ ë¡œë“œ"""
    global ner_pipeline, model_loaded
    
    if not NER_AVAILABLE:
        print("âš ï¸ NER ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ì •ê·œì‹ ëª¨ë“œ")
        return False
    
    try:
        print("ğŸ§  NER ëª¨ë¸ ë¡œë”© ì¤‘: monologg/koelectra-base-v3-naver-ner")
        print("NER ëª¨ë¸ ë°±ê·¸ë¼ìš´ë“œ ë¡œë”© ì‹œì‘...")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        model_name = "monologg/koelectra-base-v3-naver-ner"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForTokenClassification.from_pretrained(model_name)
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        ner_pipeline = pipeline(
            "ner",
            model=model,
            tokenizer=tokenizer,
            aggregation_strategy="simple",
            device=-1  # CPU ì‚¬ìš©
        )
        
        # ë¼ë²¨ ë§¤í•‘ í™•ì¸
        label_list = ['O', 'PER-B', 'PER-I', 'FLD-B', 'FLD-I', 'AFW-B', 'AFW-I', 'ORG-B', 'ORG-I', 'LOC-B']
        print(f"ë¼ë²¨ ë§¤í•‘: {label_list}...")
        print("ì¥ì¹˜ ì„¤ì •: CPU ì‚¬ìš©")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_result = ner_pipeline("í…ŒìŠ¤íŠ¸")
        print(f"NER ëª¨ë¸ ë¡œë”© ì„±ê³µ: monologg/koelectra-base-v3-naver-ner")
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(test_result)}ê°œ ì—”í„°í‹° íƒì§€")
        
        model_loaded = True
        print("NER ëª¨ë¸ ë¡œë”© ì„±ê³µ (1.4ì´ˆ)")
        return True
        
    except Exception as e:
        print(f"âŒ NER ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        model_loaded = False
        return False

def initialize_manager():
    """ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
    global manager_initialized
    
    print("ê°€ëª…í™”ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
    print("ê°€ëª…í™”ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¤‘...")
    
    try:
        # ë°ì´í„°í’€ ë¡œë“œ
        load_data_pools()
        print("ë°ì´í„°í’€ ë¡œë”© ì„±ê³µ")
        
        # NER ëª¨ë¸ ë¡œë“œ (ë°±ê·¸ë¼ìš´ë“œ)
        def load_model_async():
            load_ner_model()
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë¡œë“œ
        thread = threading.Thread(target=load_model_async)
        thread.daemon = True
        thread.start()
        
        manager_initialized = True
        print("ê°€ëª…í™”ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ===== íƒì§€ í•¨ìˆ˜ë“¤ =====
def detect_pii_items(text: str) -> List[Dict[str, Any]]:
    """PII í•­ëª© íƒì§€"""
    items = []
    
    # ì´ë©”ì¼ íƒì§€
    for match in EMAIL_PATTERN.finditer(text):
        items.append({
            "type": "ì´ë©”ì¼",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "ì •ê·œì‹"
        })
    
    # ì „í™”ë²ˆí˜¸ íƒì§€
    for match in PHONE_PATTERN.finditer(text):
        items.append({
            "type": "ì „í™”ë²ˆí˜¸",
            "value": match.group(),
            "start": match.start(),
            "end": match.end(),
            "confidence": 1.0,
            "source": "ì •ê·œì‹"
        })
    
    # ë‚˜ì´ íƒì§€
    for match in AGE_PATTERN.finditer(text):
        age_value = match.group(1)
        if 1 <= int(age_value) <= 120:
            items.append({
                "type": "ë‚˜ì´",
                "value": age_value,
                "start": match.start(),
                "end": match.start() + len(age_value),
                "confidence": 0.9,
                "source": "ì •ê·œì‹"
            })
    
    # ì´ë¦„ íƒì§€ (ì •ê·œì‹ íŒ¨í„´)
    for pattern in NAME_PATTERNS:
        for match in pattern.finditer(text):
            name = match.group(1)
            if len(name) >= 2 and name in full_name_pool:
                items.append({
                    "type": "ì´ë¦„",
                    "value": name,
                    "start": match.start(1),
                    "end": match.end(1),
                    "confidence": 0.85,
                    "source": "íŒ¨í„´"
                })
    
    # ì£¼ì†Œ íƒì§€
    for pattern in ADDRESS_PATTERNS:
        for match in pattern.finditer(text):
            address = match.group()
            if len(address) >= 3:
                items.append({
                    "type": "ì£¼ì†Œ",
                    "value": address,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.8,
                    "source": "íŒ¨í„´"
                })
    
    return items

def create_substitution_map(items: List[Dict[str, Any]]) -> Dict[str, str]:
    """ëŒ€ì²´ ë§µ ìƒì„±"""
    substitution_map = {}
    
    name_counter = 1
    age_pool_used = set()
    
    for item in items:
        original = item["value"]
        
        if original in substitution_map:
            continue
        
        if item["type"] == "ì´ë¦„":
            fake_name = f"ê¹€ê°€ëª…{name_counter}"
            name_counter += 1
            substitution_map[original] = fake_name
            
        elif item["type"] == "ë‚˜ì´":
            age = int(original)
            fake_age = random.randint(max(20, age-10), min(80, age+10))
            while fake_age in age_pool_used:
                fake_age = random.randint(20, 80)
            age_pool_used.add(fake_age)
            substitution_map[original] = str(fake_age)
            
        elif item["type"] == "ì´ë©”ì¼":
            fake_email = random.choice(email_pool)
            substitution_map[original] = fake_email
            
        elif item["type"] == "ì „í™”ë²ˆí˜¸":
            fake_phone = random.choice(phone_pool)
            substitution_map[original] = fake_phone
            
        elif item["type"] == "ì£¼ì†Œ":
            fake_address = random.choice(address_pool)
            substitution_map[original] = fake_address
    
    return substitution_map

def apply_substitutions(text: str, substitution_map: Dict[str, str]) -> str:
    """ëŒ€ì²´ ì ìš©"""
    result = text
    
    # ê¸´ ë¬¸ìì—´ë¶€í„° ëŒ€ì²´ (ê²¹ì¹˜ëŠ” ë¬¸ì œ ë°©ì§€)
    sorted_items = sorted(substitution_map.items(), key=lambda x: len(x[0]), reverse=True)
    
    for original, replacement in sorted_items:
        result = result.replace(original, replacement)
    
    return result

# ===== Flask ë¼ìš°íŠ¸ =====
@app.route("/", methods=["GET", "OPTIONS"])
def root():
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response
    
    # ë§¤ë‹ˆì € ì´ˆê¸°í™”
    if not manager_initialized:
        initialize_manager()
    
    return jsonify({
        "message": "GenAI ê°€ëª…í™”ê¸° (AenganZ ê°œì„ íŒ)", 
        "version": "2.0.0",
        "framework": "Flask (ë…ë¦½í˜•)",
        "detection_method": "NER + ì •ê·œì‹ + ë°ì´í„°í’€",
        "ner_model_loaded": model_loaded,
        "data_pools": {
            "names": len(name_pool),
            "full_names": len(full_name_pool),
            "fake_names": len(fake_name_pool),
            "emails": len(email_pool)
        },
        "timestamp": datetime.now().isoformat()
    })

@app.route("/health", methods=["GET", "OPTIONS"])
def health():
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response
    
    return jsonify({
        "status": "ì •ìƒ",
        "method": "ê°•í™”ëœ_íƒì§€",
        "ready": manager_initialized,
        "timestamp": datetime.now().isoformat()
    })

@app.route("/pseudonymize", methods=["POST", "OPTIONS"])
def pseudonymize():
    # CORS preflight ìš”ì²­ ì²˜ë¦¬
    if request.method == "OPTIONS":
        response = jsonify({"message": "CORS preflight OK"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', '*')
        response.headers.add('Access-Control-Allow-Methods', '*')
        return response
    
    # ë§¤ë‹ˆì € ì´ˆê¸°í™” í™•ì¸
    if not manager_initialized:
        initialize_manager()
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = request.get_json()
        if not data or "prompt" not in data:
            return jsonify({"error": "ìš”ì²­ì— 'prompt' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        text = data["prompt"]
        request_id = data.get("id", f"req_{int(time.time() * 1000)}")
        
        print("============================================================")
        print(f"ê°€ëª…í™” ìš”ì²­: {time.strftime('%H:%M:%S')}")
        print(f"ID: {request_id}")
        print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text}")
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not text.strip():
            return jsonify({
                "pseudonymized_text": text,
                "detection": {"contains_pii": False, "items": []},
                "substitution_map": {},
                "reverse_map": {},
                "processing_time": 0
            })
        
        start_time = time.time()
        
        print(f"ê°€ëª…í™” ì‹œì‘: {text[:50]}...")
        print("PII íƒì§€ (NER ê°„ì†Œí™” + ì •ê·œì‹ ì¤‘ì‹¬)")
        
        # PII íƒì§€
        detection_start = time.time()
        pii_items = detect_pii_items(text)
        detection_time = time.time() - detection_start
        
        print(f"PII ë¶„ì„ (ì •ê·œì‹ ì¤‘ì‹¬): {text[:50]}...")
        
        # ì¤‘ë³µ ì œê±°
        unique_items = []
        seen_values = set()
        
        for item in pii_items:
            value_key = (item['type'], item['value'])
            if value_key not in seen_values:
                unique_items.append(item)
                seen_values.add(value_key)
        
        print(f"ìµœì¢… íƒì§€ ê²°ê³¼: {len(unique_items)}ê°œ í•­ëª©")
        for i, item in enumerate(unique_items, 1):
            print(f"#{i} {item['type']}: '{item['value']}' (ì‹ ë¢°ë„: {item['confidence']:.2f}, ì¶œì²˜: {item['source']})")
        
        # ê°€ëª… í• ë‹¹
        replacement_start = time.time()
        substitution_map = create_substitution_map(unique_items)
        
        print(f"ëª…í™•í•œ ê°€ëª… í• ë‹¹ ì‹œì‘: {len(unique_items)}ê°œ í•­ëª©")
        for original, replacement in substitution_map.items():
            print(f"ëŒ€ì²´: '{original}' â†’ '{replacement}'")
        print(f"ëª…í™•í•œ ê°€ëª… í• ë‹¹ ì™„ë£Œ: {len(substitution_map)}ê°œ ë§¤í•‘")
        
        # í…ìŠ¤íŠ¸ ëŒ€ì²´
        print(f"ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ëŒ€ì²´: {len(substitution_map)}ê°œ ë§¤í•‘")
        pseudonymized_text = apply_substitutions(text, substitution_map)
        replacement_time = time.time() - replacement_start
        
        print(f"ìŠ¤ë§ˆíŠ¸ ëŒ€ì²´ ì™„ë£Œ: {len(substitution_map)}ê°œ ì ìš©")
        print(f"ì´ì „: {text}")
        print(f"ì´í›„: {pseudonymized_text}")
        
        # ì—­ë°©í–¥ ë§µ ìƒì„±
        reverse_map = {v: k for k, v in substitution_map.items()}
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time
        print(f"ì²˜ë¦¬ ì‹œê°„: íƒì§€ {detection_time:.3f}ì´ˆ, ëŒ€ì²´ {replacement_time:.3f}ì´ˆ, ì „ì²´ {total_time:.3f}ì´ˆ")
        print(f"ê°€ëª…í™” ì™„ë£Œ: {len(unique_items)}ê°œ PII ì²˜ë¦¬")
        print(f"ëŒ€ì²´ ë§µ: {substitution_map}")
        print(f"ê°€ëª…í™” ì™„ë£Œ ({len(unique_items)}ê°œ í•­ëª© íƒì§€)")
        print("ë¡œê·¸ ì €ì¥ë¨: pseudo-log.json")
        print(f"ê°€ëª…í™” ì™„ë£Œ ({len(unique_items)}ê°œ íƒì§€)")
        print(f"ëŒ€ì²´ ë§µ: {substitution_map}")
        print("============================================================")
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "pseudonymized_text": pseudonymized_text,
            "detection": {
                "contains_pii": len(unique_items) > 0,
                "items": [
                    {
                        "type": item["type"],
                        "value": item["value"],
                        "start": item["start"],
                        "end": item["end"],
                        "confidence": item["confidence"],
                        "source": item["source"],
                        "replacement": substitution_map.get(item["value"], item["value"])
                    }
                    for item in unique_items
                ],
                "model_used": "NER + ì •ê·œì‹ + ì´ë¦„í’€ + ì „ì²´ì´ë¦„í’€"
            },
            "substitution_map": substitution_map,
            "reverse_map": reverse_map,
            "performance": {
                "items_detected": len(unique_items)
            }
        }
        
        # ë¡œê·¸ ì €ì¥
        log_entry = {
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "remote_addr": request.remote_addr,
            "path": request.path,
            "input": {"id": request_id, "prompt": text},
            **result
        }
        
        append_json_to_file(LOG_FILE, log_entry)
        
        # CORS í—¤ë” ì¶”ê°€
        response = jsonify(result)
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
        
    except Exception as e:
        error_msg = f"ê°€ëª…í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"âŒ {error_msg}")
        
        response = jsonify({"error": error_msg})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500

@app.route("/prompt_logs", methods=["GET"])
def get_logs():
    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            raw = f.read()
        
        response = app.response_class(
            response=raw,
            status=200,
            mimetype="application/json; charset=utf-8"
        )
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
        
    except FileNotFoundError:
        empty = {"logs": []}
        response = app.response_class(
            response=json.dumps(empty, ensure_ascii=False),
            status=200,
            mimetype="application/json; charset=utf-8"
        )
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
    except Exception as e:
        response = jsonify({"error": f"ë¡œê·¸ ì½ê¸° ì˜¤ë¥˜: {e}"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500

@app.route("/prompt_logs", methods=["DELETE"])
def clear_logs():
    try:
        with open(LOG_FILE, "w", encoding="utf-8") as f:
            json.dump({"logs": []}, f, ensure_ascii=False)
        
        response = jsonify({"success": True, "message": "ë¡œê·¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response
    except Exception as e:
        response = jsonify({"success": False, "error": str(e)})
        response.headers.add('Access-Control-Allow-Origin', '*')
        return response, 500

if __name__ == "__main__":
    print("ğŸ­ GenAI ê°€ëª…í™”ê¸° (AenganZ ê°œì„ íŒ - ë…ë¦½í˜•)")
    print("ğŸ”§ í”„ë ˆì„ì›Œí¬: Flask (ë‹¨ì¼ íŒŒì¼ ë²„ì „)")
    print("ğŸ§  íƒì§€ ë°©ì‹: NER + ì •ê·œì‹ + ë°ì´í„°í’€")
    print("ğŸ“› ê°€ëª…í™”: ì‹¤ì œ ë°ì´í„° ëŒ€ì²´")
    print("ğŸ”„ ë³µì›: ì–‘ë°©í–¥ ë§¤í•‘")
    print("ğŸŒ ì„œë²„ ì‹œì‘ ì¤‘...")
    
    try:
        app.run(
            host="127.0.0.1",
            port=5000,
            debug=True,
            threaded=True
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì„œë²„ ì¢…ë£Œ")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()